---
title: "Simulazioni"
author: "Marco"
date: "2023-10-03"
output: html_document
---
```{r Fatte e da fare}
#7/11/2023
# ARMA nella media. R2 predittivi molto bassi. Non sono state incluse le variabili per catturare gli effetti.

# Da fare: includere le variabili. FATTO
# Provare con la stagionalita. 
# Confrontare i risultati senza e con le variabili. FATTO


#8/11/2023
# Mandato l'effeto stagionale nelle medie, senza variabili del tempo.
# Mandare con l'effetto delle variabili nel tempo. Già non c'era niente, non c'è nulla da risolvere.
# Forse dovrei provare ad includere i ritardi stagionali?. Fatto
# Ricontrollare velocemente gli R2 predittivi dell'ARMA.

# Mettere delle relazioni più complicate, tipo i quadrati.

#11/11/2023
# La componente stagionale nelle medie rompe molto i coglioni.
# Dovremmo mettere i ritardi anche nella componente stagionale nell'errore. Fatto
# Bisogna rilanciare tutte le cose con la stagionalita, perchè il giorno dell'anno era sbagliato.
# La rete neurale è stata sistemata.

# Ricontrollare la performance dell'ARMA. Fatto

## 13/11/2023
# Fatto le prove con l'andamento ARMA negli effetti dei coefficienti. Tutti i coefficienti variano.

## 16/11/2023
# Andamento stagionale nei coefficienti mandato.
# Provato a sistemare la random forest: aumentare le osservazioni aumenta leggermente la performance. 
# effetti tra -2:2, sd_WN = 20 R2 ridge =0.78, R2 rf = 0.6
# effetti tra -1.5:1.5, sd_WN = 8 R2 ridge =0.83 R2 rf =0.64
# il distacco è sempre quello.
```

```{r Reset}
rm(list=ls())
```

```{r Libraries}
library(MASS)
library(ggplot2)
library(glmnet)
library(ranger)
library(gbm)
library(sm)
library(gridExtra)

## Per generare le matrici di varianbza e covarianza.
#library(clusterGeneration)
library(Matrix)


# Calcolo parallelo
library(foreach)
library(doParallel)

# Reti neurali
library(keras)
library(tensorflow)
```

```{r FUNZIONI senza cambiamento nel tempo}

## Matrice di varianza e covarianza con correlazione.
get_Sigma <- function(nvar_input, rho) {
  Sigma <- matrix(runif(nvar_input*nvar_input,min=-rho,max=rho),nrow=nvar_input, ncol=nvar_input)
  Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
  diag(Sigma) <- rep(1, nvar_input)
  Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)
  return(list(mat = Sigma$mat, convergence = Sigma$converged))
}

## Aggiunge outliers
add.outliers <- function(output, freq) {
  sd <- sd(output)
  num.out <- floor(length(output)*freq)
  ind <- sample(1:length(output), num.out, replace=FALSE )
  output[ind] <- (abs(output[ind]) + 3*sd) * sign(output[ind])
  return(output)
}




## Noise normale
noise.lin <- function(n) {
  return(rnorm(n = n,mean = 0, sd = sqrt(sd_WN)))
}

## Output funzione lineare
lin.out <- function(input) {
  ## coef[1] è l'intercetta, il resto i coefficienti.
  coef <- runif(ncol(input) + 1, min = -5, max = 5)
  output <- apply(input,1,function(x) coef[1] + t(as.matrix(x))%*%as.matrix(coef[2:length(coef)]))
  cat("\nMain effects: ",coef)
  output <- output + noise.lin(nrow(input))
  return(output)
}




## Relazione quadratica con l'output
quad.out <- function(input) {
  
  ## Effetto lineare della variabile
  main.eff <- lin.out(input)
  
  ## coef.sq sono per la parte quadratica.
  coef.sq <- runif(ncol(input), min = -5, max = 5)
  sq.eff <- apply(input,1,function(x) t(as.matrix(x^2))%*%as.matrix(coef.sq))
  
  cat("\nSquare effects:",coef.sq)
  ## Output come funzione dei quadrati e degli effetti principali.
  output <- main.eff + sq.eff
  return(output)
}


cub.out <- function(input) {
  
  ## Effetto lineare della variabile
  #main.eff <- quad.out(input)
  main.eff <- 0
  
  ## coef.sq sono per la parte quadratica.
  coef.cub <- runif(ncol(input), min = -5, max = 5)
  cub.eff <- apply(input,1,function(x) t(as.matrix(x^3))%*%as.matrix(coef.cub))
  
  cat("\nCubic effects:",coef.cub)
  ## Output come funzione dei quadrati e degli effetti principali.
  output <- main.eff + cub.eff + noise.lin(nrow(input))
  return(output)
}


## Relazione lineare con interazioni.
#n_int = 8

interaction.out <- function(input) {
  
  ## Definisco gli oggetti necessari all'interno
  ints <- matrix("V0", nrow=n_int, ncol=2)
  var_inter <- rep(0,ncol(input))
  names(var_inter) <- colnames(input)

  coef.int <- runif(n_int, min = -5, max = 5)
  
  main.eff <- lin.out(input)
  
  ## Interazioni casuali tra variabili
  ## Scegliamo le combinazioni tra cui inserire interazione.
  
  int.eff <- 0
  
  for (i in 1:n_int){
    #cat("\n",i)
    # i=1
    
    ## Primo termine di interazione
    possible_first <- names(which(var_inter < (nvar_input - 1)))
    first <- sample(possible_first,1)
    
    ## Secondo termine di interazione
    possible_second <- setdiff(colnames(input), first)
    possible_second <- setdiff(possible_second,ints[ints[,1] == first,2])
    possible_second <- setdiff(possible_second,ints[ints[,2] == first,1])
    second <- sample(possible_second,1)
    
    ## Aggiorno la matrice di controllo.
    ints[i,] <- c(first,second)
    ## E aggiorno il numero di interazioni per ciascun oggetto.
    var_inter[names(var_inter) == first] <- var_inter[names(var_inter) == first] + 1
    var_inter[names(var_inter) == second] <- var_inter[names(var_inter) == second] + 1
    
    
    primo_t <- colnames(input) == first
    secondo_t <- colnames(input) == second
    #x=input[1,]
    int.eff <- int.eff + apply(input,1,function(x) coef.int[i]*x[primo_t]*x[secondo_t] )
  }
  
  cat("\nInteract effects:",coef.int)
  output <- main.eff + int.eff
  return(list(output = output,interazioni = ints,singole_int = var_inter))

}


## Funzione pre creare l'ultimo dataset problematico
int.quad.out <- function(input) {
  
  ## Interazione
  int.eff <- interaction.out(input)
  ## coef.sq sono per la parte quadratica.
  coef.sq <- runif(ncol(input), min = -5, max = 5)
  sq.eff <- apply(input,1,function(x) t(as.matrix(x^2))%*%as.matrix(coef.sq))
  
  cat("\nSquare effects:",coef.sq)
  ## Output come funzione dei quadrati e degli effetti principali.
  output <- int.eff$output + sq.eff
  return(list(output = output, interazioni = int.eff$interazioni, singole_int = int.eff$singole_int))
}


## Crea il dataset.
create.data <- function(output,input, daily_obs) {
  data <- cbind(input,output,rep(1:n_giorni,each=daily_obs))
  colnames(data)[ncol(data)] <- "Days"
  return(as.data.frame(data))
}

```

```{r Simulazioni STAZIONARIETA', nessun cambiamento nel tempo del fenomeno}

## PRIMO DATASET
## Input indipendenti
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- 150
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(4921)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input, daily_obs)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)
## R2 = 0.81


## SECONDO DATASET
## Input correlati
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 120
mu <- rep(0,nvar_input)

set.seed(119)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)
## R2 di 0.80


## DATASET DI PROVA RANDOM FOREST - Per fixare le prestazioni della random forest.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- 120
#sd_WN <- 8 perché gli effetti sono più piccoli
mu <- rep(0,nvar_input)

set.seed(119)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input)


## TERZO DATASET - effetti quadratici
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 450
mu <- rep(0,nvar_input)

set.seed(4822)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.quad <- quad.out(input)
data <- create.data(output.quad,input, daily_obs)

## Controllo varianza spiegata.
formula.quad <- paste(paste(colnames(input), collapse = " + "),"+",paste0("I(",colnames(input),"^2)", collapse =" + "))
formula.quad <- paste("output ~",formula.quad)

fit <- lm(formula.quad, data = data[,-ncol(data)])
summary(fit)  ## 0.808 di R2 del modello originale.



## TERZOb DATASET -  effetti cubici
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 2500
mu <- rep(0,nvar_input)

set.seed(88911)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

# var <- runif(nvar_input,min=1, max = 5)
# var <- diag(var)
# Sigma <- var%*%Sigma%*%var

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.cubic <- cub.out(input)
data <- create.data(output.cubic,input, daily_obs)

formula.cub <- paste(paste(colnames(input), collapse = " + "),"+",paste0("I(",colnames(input),"^2)", collapse =" + "))
formula.cub <- paste(formula.cub, "+", paste0("I(",colnames(input),"^3)", collapse =" + "))
formula.cub <- paste("output ~",formula.cub)

fit <- lm(formula.cub, data = data[,-ncol(data)])
summary(fit)   ## 0.80 iniziale.
fit <- lm(output ~., data = data[,-ncol(data)])   ## 0.47



## QUARTO DATASET - Interazioni

nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 300
mu <- rep(0,nvar_input)

set.seed(1120)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

##int_max <- factorial(nvar_input)/(factorial(2)*factorial(nvar_input-2))
n_int = 120
output.int.list <- interaction.out(input)
data <- create.data(output.int.list$output, input)

##Controllo varianza spiegata.
formula.int <- paste(paste(colnames(input), collapse = " + "))
for (i in 1:nrow(output.int.list$interazioni)) {
  term <- paste0(output.int.list$interazioni[i,], collapse=":")
  formula.int <- paste(formula.int,"+" ,term)
}
formula.int <- paste("output ~",formula.int)
fit <- lm(formula.int, data = data[,-ncol(data)])
summary(fit)  ## 0.82 di R2.



## QUARTOb DATASET - Più interazioni
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 900
mu <- rep(0,nvar_input)

set.seed(7492)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

##int_max <- factorial(nvar_input)/(factorial(2)*factorial(nvar_input-2))
n_int = 360
output.int.list <- interaction.out(input)
data <- create.data(output.int.list$output,input)

##Controllo varianza spiegata.
formula.int <- paste(paste(colnames(input), collapse = " + "))
for (i in 1:nrow(output.int.list$interazioni)) {
  term <- paste0(output.int.list$interazioni[i,], collapse=":")
  formula.int <- paste(formula.int,"+" ,term)
}
formula.int <- paste("output ~",formula.int)
fit <- lm(formula.int, data = data[,-ncol(data)])
summary(fit)  ## 0.81


## QUINTO DATASET - Correlazione con il termine d'errore.

nvar_input <- 120
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 200
mu <- rep(0,nvar_input)

set.seed(4093)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input)
## Escludiamo 20 variabili
X <- setdiff(colnames(input), sample(colnames(input),floor(0.3*nvar_input)))
fit <- lm(output~., data = data[,-ncol(data)])
fit <- lm(output~., data = data[,c(X,"output")])  ## 0.83 R2.
summary(fit)



## SETTIMO - correlazione ed interazione con la componente di errore.
nvar_input <- 120
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 150
mu <- rep(0,nvar_input)

set.seed(3995)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

n_int = 120
output.int.list <- interaction.out(input)
data <- create.data(output.int.list$output, input)

X <- setdiff(colnames(input), sample(colnames(input),floor(0.3*nvar_input)))
setdiff(colnames(input),X)

fit <- lm(output~., data = data[,c(X,"output")])  ## 0.49 R2, con solo un sottoinsieme delle variabili.
summary(fit)

##Controllo varianza spiegata dalle variabili rese disponibili.
formula.int <- paste(X, collapse = " + ")
selected.int <- output.int.list$interazioni
for (i in 1:nrow(output.int.list$interazioni)) {
  term <- paste0(output.int.list$interazioni[i,], collapse=":")
  formula.int <- paste(formula.int,"+" ,term)
}
formula.int <- paste("output ~",formula.int)
fit <- lm(formula.int, data = data[,-ncol(data)])   ## 0.91 di R2, con tutte le variabili nel modello e tutte le interazioni.




## SESTO DATASET - Variabili di disturbo

nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 150
mu <- rep(0,nvar_input)

set.seed(6589)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
disturbo <- sample(colnames(input),floor(0.25*nvar_input))
output.lin <- lin.out(input[,setdiff(colnames(input),disturbo)])
data <- create.data(output.lin,input)
## Escludiamo 20 variabili
fit <- lm(output~., data = data[,-ncol(data)])
fit <- lm(output~., data = data[,c(setdiff(colnames(input),disturbo),"output")])  ## 0.80 circa di R2.
summary(fit)




## SESTOb DATASET - Più variabili di disturbo
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 90
mu <- rep(0,nvar_input)

set.seed(43223)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
disturbo <- sample(colnames(input),floor(0.50*nvar_input))
output.lin <- lin.out(input[,setdiff(colnames(input),disturbo)])
data <- create.data(output.lin,input)
## Escludiamo 20 variabili
fit <- lm(output~., data = data[,-ncol(data)])
fit <- lm(output~., data = data[,c(setdiff(colnames(input),disturbo),"output")])
summary(fit)  ## 0.80 R2.



## OTTAVO DATASET - Outliers
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 120
mu <- rep(0,nvar_input)

set.seed(2377)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)

## outliers
sd <- sd(output.lin)
num.out <- floor(length(output.lin)*0.01)  ## 182 outliers, 1 osservazione su 100.
ind <- sample(1:length(output.lin), num.out, replace=FALSE )
output.lin[ind] <- (abs(output.lin[ind]) + 3*sd) * sign(output.lin[ind])

data <- create.data(output.lin,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)  ## 0.75 R2, si è ridotto per via degli outliers, da 0.82 iniziale.


## NONO DATASET - Più outliers
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 120
mu <- rep(0,nvar_input)

set.seed(2377)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)

## outliers
sd <- sd(output.lin)
num.out <- floor(length(output.lin)*0.05)  ## 900 circa outliers, 1 osservazione su 100.
ind <- sample(1:length(output.lin), num.out, replace=FALSE )
output.lin[ind] <- (abs(output.lin[ind]) + 3*sd) * sign(output.lin[ind])

data <- create.data(output.lin,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)     ## 0.61 R2.




## DECIMO DATASET - Sparsità delle feature
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
noise_Sigma <- 20       ## Non so se mi convince questo
mu <- rep(0,nvar_input)

set.seed(751)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

## Renderle sparse
sparse <- 0.95
for (h in 1:ncol(input)) {
  input[sample(1:nrow(input),floor(nrow(input)*sparse)),h] <- 0
}
output.lin <- lin.out(input)
data <- create.data(output.lin,input)

fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)  ## 0.63 R2.



## UNDICESIMO DATASET - Pochi dati
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 2
noise_Sigma <- 180
mu <- rep(0,nvar_input)

set.seed(60012)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input)

fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)     ## R2 iniziale di 0.82


## DODICESIMO DATASET - ancora meno dati
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 1
noise_Sigma <- 280
mu <- rep(0,nvar_input)

set.seed(1262)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input)

fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)   ## 0.82 R2.


## TREDICESIMO DATASET - meno dati + relazione più complessa.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 1
noise_Sigma <- 600
mu <- rep(0,nvar_input)

set.seed(44924)
Sigma <- matrix(runif(nvar_input*nvar_input,min=-0.7,max=0.7),nrow=nvar_input)
Sigma[lower.tri(Sigma)] <- t(Sigma)[lower.tri(Sigma)]
diag(Sigma) <- rep(1, nvar_input)
Sigma <- nearPD(Sigma, corr =TRUE, base.matrix = TRUE)$mat

input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.quad <- quad.out(input)
data <- create.data(output.quad,input, daily_obs)

## Controllo varianza spiegata.
formula.quad <- paste(paste(colnames(input), collapse = " + "),"+",paste0("I(",colnames(input),"^2)", collapse =" + "))
formula.quad <- paste("output ~",formula.quad)

fit <- lm(formula.quad, data = data[,-ncol(data)])
summary(fit)  ## 0.8 R2.




## DATASET PROBLEMI - Relazioni quadratiche + interazioni + outliers + correlazione e interazione con l'errore + variabili di disturbo.
nvar_input <- 120
n_giorni <- 1825
daily_obs <- 10
n_int = 120
noise_Sigma <- 1       
mu <- rep(0,nvar_input)
set.seed(8099)

Sigma <- get_Sigma(nvar_input)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma$mat))

vars <- colnames(input)
disturbo <- sample(vars, floor(nvar_input*0.2))
errore <- sample(setdiff(vars,disturbo),floor(nvar_input*0.15))
X <- setdiff(vars, errore)

output <- int.quad.out(input[,setdiff(vars,disturbo)])

## Performance modello
formula <- paste(paste(X, collapse = " + "),"+",paste0("I(",X,"^2)", collapse =" + "))
formula <- paste("output ~",formula)

for (i in 1:nrow(output$interazioni)) {
  if((output$interazioni[i,1]  %in% X) & (output$interazioni[i,2] %in% X)) {
    term <- paste0(output$interazioni[i,], collapse=":")
    formula <- paste(formula,"+" ,term)
  }
}


# aggiunta degli outliers
output <- add.outliers(output$output, freq = 0.01)
data <- create.data(output,input,daily_obs)


fit <- lm(formula, data=data[,X])
summary(fit)  ## Circa 0.78.



```

```{r FUNZIONI per le variazioni stazionarie}

## 
noise.ARMA <- function(n, coefAR, coefMA, d, sd) {
  arima.sim(n = n, list(order = c(length(coefAR), d, length(coefMA)),ar = coefAR, ma = coefMA), sd = sd)
}

## L'oggetto di cui fare il plot.
noiseARMA.plot <- function(noise, p =1, q=0) {
  par(mfrow=c(3,1))
  plot(x = 1:length(noise), y = noise, ylab="", xlab="giorni", type ="l")
  title(paste0("Grafico dell'errore ARMA(",p,",",q,")"))
  acf(noise, lag.max=length(noise)/4)
  pacf(noise,lag.max=length(noise)/4)
}

## Somma l'errore ottenuto dal modello ARMA (giornaliero) ad un errore WN.
noise.WN <- function(n) {
  rnorm(n = n_giorni*daily_obs, mean = 0, sd = sd_WN)
}

## Plotta l'output e le componenti dell'errore.
allNoise.plot <- function(noiseARMA, noiseWN, output) {
  par(mfrow=c(3,1))
  plot(1:length(noiseARMA),noiseARMA,ylab="", xlab="giorni", type="l")
  title("Processo ARMA per l'errore")
  plot(1:length(noiseWN),noiseWN,ylab="", xlab="giorni", type="l")
  title("Processo WN da aggiungere all'errore")
  plot(1:length(output),output,ylab="", xlab="giorni", type="l")
  title("Output senza errore")
}

## Output funzione lineare
lin.out <- function(input) {
  ## coef[1] è l'intercetta, il resto i coefficienti.
  coef <- runif(ncol(input) + 1, min = -5, max = 5)
  output <- apply(input,1,function(x) coef[1] + t(as.matrix(x))%*%as.matrix(coef[2:length(coef)]))
  cat("\nMain effects: ",coef)
  output <- output
  return(output)
}

create.data <- function(output, input) {
  data <- cbind(input,output,rep(1:n_giorni,each=daily_obs))
  colnames(data)[ncol(data)] <- "Days"
  return(data)
}

get_daily_means <- function(data) {
  data.dm <- data.frame(Days = unique(data$Days))
  data.dm$output.mean <- apply(as.matrix(data.dm$Days),1,function(x) mean(data$output[data$Days == x])) 
  data.dm$output.mean.lag1 <-  c(NA,data.dm$output.mean[1:(nrow(data.dm)-1)])
  data.dm$output.mean.lag2 <-  c(rep(NA,2),data.dm$output.mean[1:(nrow(data.dm)-2)])
  cor.l1 <- cor(data.dm$output.mean[-1], data.dm$output.mean.lag1[-1])
  cor.l2 <- cor(data.dm$output.mean[-c(1,2)], data.dm$output.mean.lag2[-c(1,2)])
  return(list(data =data.dm, cor.l1 = cor.l1,cor.l2 = cor.l2))
}

  
output_plot <- function(data) {
  
  ## Media mobile
  mobile_average <- function(order = m, data = df.for.plot){
    ma <- apply(as.matrix(data$Days),1,function(x) mean(data$output.mean[data$Days >= (x-order) & data$Days <= (x+order)]))
    ma[c(1:order,(length(ma)-order+1):length(ma))] <- NA
    return(ma)
  }
    
  m <- 15
  df.for.plot <- get_daily_means(data)$data[,c("Days","output.mean")]
  df.for.plot$mobile_av <- mobile_average(m, df.for.plot)
  
  plot <- ggplot(data=df.for.plot, aes(x=Days)) + geom_line(aes(y = output.mean, color = "output"), size=1.2) + 
    geom_line(aes(y = mobile_av, color = "mobile_av")) + 
    scale_color_manual(
    values = c("output" = "lightblue", "mobile_av" = "black"),
    labels = c("Media giornaliera", "Media mobile"),
    breaks = c("output", "mobile_av")) + 
      ggtitle("Variabile risposta simulata - Medie giornaliere e media mobile")
  
  return(plot)
}

```

```{r Simulazioni STAZIONARIETA ARMA}
sample(1000:10000,1)

## Processo ARMA giornaliero AR=0.6, + errore gionaliero.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(50)
sd_noiseARMA <- sqrt(85)
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(3633)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output <- lin.out(input)

## Per sporcare l'output, processo stazionario giornaliero.
noiseARMA <- noise.ARMA(n = n_giorni, coefAR = c(0.6), coefMA = c(0), d=0, sd=sd_noiseARMA)
noiseARMA.plot(noiseARMA, p = 1, q = 0)

## Per aggiungere degli errori all'interno della giornata.
noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(noiseARMA,each=daily_obs) + noiseWN

allNoise.plot(noiseARMA, noiseWN, output)

output.final <- output + noiseTOT

data <- create.data(output.final,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)   ## 0.8 R2


## Processo ARMA giornaliero, AR=0.9, + errore gionaliero.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(50)
sd_noiseARMA <- sqrt(70)
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(1110)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output <- lin.out(input)

## Per sporcare l'output, processo stazionario giornaliero.
noiseARMA <- noise.ARMA(n = n_giorni, coefAR = c(0.9), coefMA = c(0), d=0, sd=sd_noiseARMA)
noiseARMA.plot(noiseARMA, p = 1, q = 0)

## Per aggiungere degli errori all'interno della giornata.
noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(noiseARMA,each=daily_obs) + noiseWN

allNoise.plot(noiseARMA, noiseWN, output)

output.final <- output + noiseTOT

data <- create.data(output.final,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)   ## 0.63 di R2, con le stesse specifiche. 0.66 riducendo la variabilità.





## DATASET 3 - Includiamo i primi ritardi della risposta.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(50)
sd_noiseARMA <- sqrt(85)
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(5160)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output <- lin.out(input)

## Per sporcare l'output, processo stazionario giornaliero.
noiseARMA <- noise.ARMA(n = n_giorni, coefAR = c(0.6), coefMA = c(0), d=0, sd=sd_noiseARMA)
noiseARMA.plot(noiseARMA, p = 1, q = 0)

## Per aggiungere degli errori all'interno della giornata.
noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(noiseARMA,each=daily_obs) + noiseWN

allNoise.plot(noiseARMA, noiseWN, output)

output.final <- output + noiseTOT

data <- create.data(output.final,input)

# Studiamo il dataframe delle medie
data.dm <- data.frame(Days = unique(data$Days))
data.dm$output.mean <- apply(as.matrix(data.dm$Days),1,function(x) mean(data$output[data$Days == x])) 
data.dm$output.mean.lag1 <-  c(NA,data.dm$output.mean[1:(nrow(data.dm)-1)])
data.dm$output.mean.lag2 <-  c(rep(NA,2),data.dm$output.mean[1:(nrow(data.dm)-2)])
cor(data.dm$output.mean[-1], data.dm$output.mean.lag1[-1])  ## 0.38 di correlazione.
cor(data.dm$output.mean[-c(1,2)], data.dm$output.mean.lag2[-c(1,2)]) ## 0.20 di correlazione.
# Un analista includerebbe questa informazione nel modello, una dipendenza che non dovrebbe esserci.

# Includo nei dati la media del giorno precedente.
data$output.l1 <- NA
data$output.l2 <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$output.mean.lag1[data.dm$Days == i]
  data$output.l2[data$Days == i] <- data.dm$output.mean.lag2[data.dm$Days == i]
}

## Tutta la variabilità dell'errore precedente viene catturata dall'inclusione dei ritardi.
# Proviamo con altri processi stazionari?


## DATASET 4 - Processo non osservato più complesso.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(50)
sd_noiseARMA <- sqrt(70)
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(7014)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output <- lin.out(input)

coef.ma <- round(runif(4),2)
noiseARMA <- noise.ARMA(n = n_giorni, coefAR = c(), coefMA = coef.ma, d=0, sd=sd_noiseARMA)
## Questo effetto dovrebbe essere più difficile da spiegare attraverso i ritardi.
noiseARMA.plot(noiseARMA, p = 0, q = 4)

## Per aggiungere degli errori all'interno della giornata.
noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(noiseARMA,each=daily_obs) + noiseWN

allNoise.plot(noiseARMA, noiseWN, output)

output.final <- output + noiseTOT

data <- create.data(output.final,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)  ## 0.7 del modello originale.

## Com'è la correlazione?
data.dm <- data.frame(Days = unique(data$Days))
data.dm$output.mean <- apply(as.matrix(data.dm$Days),1,function(x) mean(data$output[data$Days == x])) 
data.dm$output.mean.lag1 <-  c(NA,data.dm$output.mean[1:(nrow(data.dm)-1)])
data.dm$output.mean.lag2 <-  c(rep(NA,2),data.dm$output.mean[1:(nrow(data.dm)-2)])
cor(data.dm$output.mean[-1], data.dm$output.mean.lag1[-1])  ## 0.68 di correlazione.
cor(data.dm$output.mean[-c(1,2)], data.dm$output.mean.lag2[-c(1,2)])# 0.44 di correlazione.


## DATASET 5 - Processo non osservato ARMA(2,2), così combiniamo i problemi e vediamo che cosa viene.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(50)
sd_noiseARMA <- sqrt(10)
mu <- rep(0,nvar_input)

set.seed(8869)
Sigma <- get_Sigma(nvar_input, rho=0.15)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma$mat))
output <- lin.out(input)

## Per sporcare l'output, processo stazionario giornaliero.
coef.MA <- round(runif(2),2)
noiseARMA <- noise.ARMA(n = n_giorni, coefAR = c(0.6,0.3), coefMA = coef.MA, d=0, sd=sd_noiseARMA)
noiseARMA.plot(noiseARMA, p = 2, q = 2)

## Per aggiungere degli errori all'interno della giornata.
noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(noiseARMA,each=daily_obs) + noiseWN

allNoise.plot(noiseARMA, noiseWN, output)

output.final <- output + noiseTOT

data <- create.data(output.final,input)
fit <- lm(output~., data = data[,-ncol(data)])
summary(fit)  ## R2 di 0.75.

data.dm <- get_daily_means(data)
data.dm$cor.l1  ## Forte correlazione ai primi ritardi.

data$output.l1 <- NA
data$output.l2 <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$data$output.mean.lag1[data.dm$data$Days == i]
  data$output.l2[data$Days == i] <- data.dm$data$output.mean.lag2[data.dm$data$Days == i]
}
plot <- output_plot(data)

```

```{r FUNZIONI stagionali}
allNoise.stag.plot <- function(processo_latente, noiseWN, output) {
  par(mfrow=c(2,1))
  plot(1:length(processo_latente),processo_latente,ylab="", xlab="giorni", type="l")
  title("Processo sottostante")
  #plot(1:length(noiseWN),noiseWN,ylab="", xlab="giorni", type="l")
  #title("Processo WN da aggiungere all'errore")
  plot(1:length(output),output,ylab="", xlab="giorni", type="l")
  title("Output")
}

all.plot <- function(processo_latente, noiseWN, output) {
  par(mfrow=c(2,1))
  plot(1:length(processo_latente),processo_latente,ylab="", xlab="giorni", type="l")
  title("Processo sottostante")
  #plot(1:length(noiseWN),noiseWN,ylab="", xlab="giorni", type="l")
  #title("Processo WN da aggiungere all'errore")
  plot(1:length(output),output,ylab="", xlab="giorni", type="l")
  title("Output")
}
```

```{r Simulazioni VARIAZIONI STAGIONALI NELL'ERRORE}
sample(1000:10000, 1)

## PRIMO DATASET - Andamento stagionale nel termine d'errore
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(25)
sd_stag <- sqrt(20)
mu <- rep(0,nvar_input)

set.seed(6280)
Sigma <- get_Sigma(nvar_input, rho=0.15)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma$mat))
output <- lin.out(input)

# Simulazione dell'andamento stagionale
a <- 7
Gamma <- c(sort(round(runif(a, min=0, max=100),2), decreasing =TRUE), sort(round(runif(24-a, min=0, max=75),2),decreasing=FALSE))
Gamma <- Gamma - mean(Gamma)
plot(1:24,Gamma)

#Il numero di di valori di cui ho bisogno.
n_gamma <- n_giorni/365*24 + 6
for(i in 25:n_gamma) {
    Gamma <- c(Gamma,-sum(Gamma[(i-23):(i - 1)]))
}

Gamma <- Gamma + rnorm(length(Gamma), sd = sd_stag)

plot(1:length(Gamma),Gamma)

x <- floor(rep(seq(1,365, length.out = 24),5) + rep(365*c(0,1,2,3,4), each=24))
x <- c(c(-44,-29,-14),x,c(1840,1855,1870))
fit = loess(Gamma ~ x, span=0.065, degree=2)
  
stag_latente <- predict(fit, data.frame(x = 1:n_giorni))
  
plot <- ggplot(data=data.frame(stag_latente = stag_latente, days = 1:n_giorni), aes(x=days, y = stag_latente)) + geom_line(stat="identity")
plot

noiseWN <- noise.WN(n_giorni*daily_obs) 
noiseTOT <- rep(stag_latente, each = daily_obs) + noiseWN
output.final <- output + noiseTOT

allNoise.stag.plot(stag_latente, noiseWN, output.final)


data <- create.data(output.final,input)
plot <- output_plot(data)
plot

fit <- lm(output~., data = data[,-ncol(data)])
summary(fit) ## 0.48 di R2 iniziale.


## SECONDO DATASET - Dando la possibilità ai modelli di catturare la componente stagionale.
# Sugli stessi dati di prima


## Inseriamo una funzione quadratica e una dummy del tempo.
data$day_of_year <- rep(rep(1:365,each=daily_obs),5) ## La stagionalità ha l'ordine corretto ora.
data$day_of_year2 <- data$day_of_year^2
# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- m1
data$month_of_year <- factor(data$month_of_year)

## Inserisco i ritardi
data.dm <- get_daily_means(data)
data.dm$cor.l1  ## Forte correlazione ai primi ritardi. 0.90.
data.dm$data
data$output.l1 <- NA
data$output.l2 <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$data$output.mean.lag1[data.dm$data$Days == i]
  data$output.l2[data$Days == i] <- data.dm$data$output.mean.lag2[data.dm$data$Days == i]
}
plot <- output_plot(data)

## Inserire ritardi ulteriori? Se il modello la cattura bene, non credo sia necessario.

```

```{r FUNZIONI VARIAZIONI STAZIONARIE nelle MEDIE}

noise_ARMA <- function(n, coefAR, coefMA, d, sd) {
  arima.sim(n = n, list(order = c(length(coefAR), d, length(coefMA)),ar = coefAR, ma = coefMA), sd = sd)
}

noiseARMA.plot <- function(noise, p =1, q=0) {
  par(mfrow=c(3,1))
  plot(x = 1:length(noise), y = noise, ylab="", xlab="giorni", type ="l")
  title(paste0("Grafico dell'errore ARMA(",p,",",q,")"))
  acf(noise, lag.max=length(noise)/4)
  pacf(noise,lag.max=length(noise)/4)
}

input_ARMA <- function(coefAR, coefMA, d, sd_ARMA, arma_var) {
  
  arma_average <- noise_ARMA(n_giorni, coefAR, coefMA, d, sd_ARMA)
  noiseARMA.plot(arma_average, p = length(coefAR), q = length(coefMA))
  
  
  plot <- ggplot(data=data.frame(arma = arma_average, days = 1:n_giorni), aes(x=days, y =arma_average)) + geom_line(stat="identity")
  
  mu <- rep(0,nvar_input)
  input <- NULL
  for (j in 1:n_giorni) {
    mu[arma_var] <- arma_average[j]
    input <- rbind(input,mvrnorm(n = daily_obs, mu = mu, Sigma = Sigma))
  }
  
  return(list(input=input, plot=plot, arma = arma_average))

}


input_stag <- function(sd_stag, stag_var, n_anni = 6) {
  
  ## Generiamo 24 valori.
  a <- 8
  Gamma <- c(sort(round(runif(a, min=0, max=30),2), decreasing =FALSE), sort(round(runif(24-a, min=0, max=20),2),decreasing=TRUE))
  Gamma <- Gamma - mean(Gamma)
  plot(1:24,Gamma)

  #Il numero di di valori di cui ho bisogno.
  n_gamma <- n_giorni/365*24 + 6
  for(i in 25:n_gamma) {
      Gamma <- c(Gamma,-sum(Gamma[(i-23):(i - 1)]))
  }
  
  Gamma <- Gamma + rnorm(length(Gamma), sd=sd_stag)
  plot(1:length(Gamma),Gamma)

  x <- floor(rep(seq(1,365, length.out = 24),n_anni) + rep(365*c(0:(n_anni-1)), each=24))
  x <- c(c(-44,-29,-14),x,c(1840,1855,1870))
  fit = loess(Gamma ~ x, span=0.065, degree=2)
  


  stag_average <- predict(fit, data.frame(x = 1:n_giorni))
  
  plot <- ggplot(data=data.frame(stag_average = stag_average, days = 1:n_giorni), aes(x=days, y = stag_average)) + geom_line(stat="identity")
  
  mu <- rep(0,nvar_input)
  input <- NULL
  for (j in 1:n_giorni) {
    mu[stag_var] <- stag_average[j]
    input <- rbind(input,mvrnorm(n = daily_obs, mu = mu, Sigma = Sigma))
  }
  
  return(list(input=input, plot=plot, stag = stag_average))
}

all.plot <- function(processo_latente, noiseWN, output) {
  par(mfrow=c(3,1))
  plot(1:length(processo_latente),processo_latente,ylab="", xlab="giorni", type="l")
  title("Processo sottostante")
  plot(1:length(noiseWN),noiseWN,ylab="", xlab="giorni", type="l")
  title("Processo WN da aggiungere all'errore")
  plot(1:length(output),output,ylab="", xlab="giorni", type="l")
  title("Output")
}

```

```{r Simulazioni VARIAZIONI STAZIONARIE NELLE MEDIE}
sample(1000:10000,1)

## PRIMO DATASET - Andamento ARMA nella media degli input.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(800)
sd_ARMA <- sqrt(2)

set.seed(5765)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
coefAR <- c(0.6, 0.3)
coefMA <- c(0.8,0.5)
# Sottocampione di variabili a cui dare andamento arma.
arma_var <- sample(1:nvar_input, floor(nvar_input*1))

input <- input_ARMA(coefAR, coefMA, d=0, sd_ARMA, arma_var = arma_var)
arma_ave <- input$arma
arma_plot <- input$plot
input <- as.data.frame(input$input)
noiseWN <- noise.WN(n_giorni*daily_obs)
output <- lin.out(input)
output.final <- output + noiseWN
allNoise.plot(arma_ave, noiseWN, output)
allNoise.plot(arma_ave, noiseWN, output.final)
  
data <- create.data(output.final,input)
plot <- output_plot(data)
plot


# C'è correlazione temporale?
data.dm <- get_daily_means(data)
data.dm$cor.l1   ## 0.95 di correlazione giornaliera.
data.dm$cor.l2  ## 0.89 di correlazione al secondo ritardo.

data$day_of_year <- rep(rep(1:365,each=daily_obs),5)
data$day_of_year2 <- data$day_of_year^2

# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- rep(m1, each =daily_obs)
data$month_of_year <- factor(data$month_of_year)

## Inserisco i ritardi
data$output.l1 <- NA
data$output.l2 <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$data$output.mean.lag1[data.dm$data$Days == i]
  data$output.l2[data$Days == i] <- data.dm$data$output.mean.lag2[data.dm$data$Days == i]
}
```

```{r Simulazioni VARIAZIONI STAGIONALI NELLE MEDIE}
sample(1000:10000,1)
## PRIMO DATASET - Andamento stagionale nella media degli input.
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(300)
sd_stag <- sqrt(15)

set.seed(1557)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat

# Sottocampione di variabili a cui dare andamento stagionale
stag_var <- sample(1:nvar_input, floor(nvar_input*1))

input <- input_stag(sd_stag, stag_var = stag_var, n_anni = 5)

stag_plot <- input$plot
stag <- input$stag
input <- as.data.frame(input$input)
output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output + noiseWN

data <- create.data(output.final,input)
plot <- output_plot(data)
all.plot(stag, noiseWN, output.final)
all.plot(stag, noiseWN, output)


## SECONDO DATASET - AGGIUGERE LA DIPENDENZA TEMPORALE
data.dm <- get_daily_means(data)
data.dm$cor.l1   ## 0.95 di correlazione giornaliera.
data.dm$cor.l2  ## 0.89 di correlazione al secondo ritardo.

data$day_of_year <- rep(rep(1:365,each=daily_obs),5) ## La stagionalità ha l'ordine corretto ora.
data$day_of_year2 <- data$day_of_year^2

# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- rep(m1, each =daily_obs)
data$month_of_year <- factor(data$month_of_year)

## Inserisco i ritardi
data$output.l1 <- NA
data$output.l2 <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$data$output.mean.lag1[data.dm$data$Days == i]
  data$output.l2[data$Days == i] <- data.dm$data$output.mean.lag2[data.dm$data$Days == i]
}








## TERZO DATASET - Ritardi stagionali
nvar_input <- 80
n_giorni <- 365*6
daily_obs <- 10
sd_WN <- sqrt(160)
sd_stag <- sqrt(12)

set.seed(6044)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat

# Sottocampione di variabili a cui dare andamento stagionale
stag_var <- sample(1:nvar_input, floor(nvar_input*1))

input <- input_stag(sd_stag, stag_var = stag_var, n_anni = 6)

stag_plot <- input$plot
stag <- input$stag
input <- as.data.frame(input$input)
output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output + noiseWN

data <- create.data(output.final,input)
plot <- output_plot(data)
plot
all.plot(stag, noiseWN, output.final)
all.plot(stag, noiseWN, output)

## Aggiungiamo i ritardi stagionali.
data.dm <- get_daily_means(data)
data.dm$cor.l1   ## 0.85 di correlazione giornaliera.
data.dm$cor.l2  ## 0.84 di correlazione al secondo ritardo.

data$day_of_year <- rep(rep(1:365,each=daily_obs),6) ## La stagionalità ha l'ordine corretto ora.
data$day_of_year2 <- data$day_of_year^2

# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- rep(m1, each =daily_obs)
data$month_of_year <- factor(data$month_of_year)



# Controlliamo quanto sono correlati i ritardi stagionali.
data.dm$data
data.dm$data$lags <- NA
for (i in 366:max(data.dm$data$Days)) {
  data.dm$data$lags[data.dm$data$Days == (i)] <- data.dm$data$output.mean[data.dm$data$Days == (i - 365)]
}
cor(data.dm$data[data.dm$data$Days>=366,c("output.mean","lags")])  ## C'è una forte correlazione al ritardo stagionale (365).


## Inserisco i ritardi, anche stagionali
data$output.l1 <- NA
data$output.l2 <- NA
data$output.ls <- NA
for (i in unique(data$Days)) {
  data$output.l1[data$Days == i] <- data.dm$data$output.mean.lag1[data.dm$data$Days == i]
  data$output.l2[data$Days == i] <- data.dm$data$output.mean.lag2[data.dm$data$Days == i]
}

## Inserisco i ritardi stagionali.
for (i in 366:max(data$Days)) {
  data$output.ls[data$Days == i] <- data.dm$data$output.mean[data.dm$data$Days == (i - 365)]
}

# Togliamo il primo anno, che serviva per il lag stagionale.
data <- data[data$Days>=366,]
data$Days <- rep(1:1825, each=daily_obs)

```

```{r Stagionalita formula}
## Processo stagionale nella componente di errore.
#https://stats.stackexchange.com/questions/125946/generate-a-time-series-comprising-seasonal-trend-and-remainder-components-in-r
## 3 componenti: latent seasonal, latent drift, latent level. Sommano alla serie osservata.
# Latent drift: Beta_t = Beta_t-1 + gugma_t
# Latent seasonality: Gamma_t = - Gamma_t-1 - ..... - Gamma_t-s + omega_t
# Latent level: Mu_t = Mu_t-1 + Beta_t-1 + epsilon_t
```

```{r FUNZIONI ANDAMENTO negli EFFETTI}
## Genera un ARMA
noise.WN <- function(n) {
  rnorm(n = n_giorni*daily_obs, mean = 0, sd = sd_WN)
}

noise_ARMA <- function(n, coefAR, coefMA, d, sd) {
  arima.sim(n = n, list(order = c(length(coefAR), d, length(coefMA)),ar = coefAR, ma = coefMA), sd = sd)
}

processo_STAG <- function(n,sd_stag,a) {
  ## Generiamo 24 valori.
  Gamma <- c(sort(round(runif(a, min=0, max=2.8),2), decreasing =FALSE), sort(round(runif(24-a, min=0, max=1.6),2),decreasing=TRUE))
  Gamma <- Gamma - mean(Gamma)
  plot(1:24,Gamma)

  #Il numero di di valori di cui ho bisogno.
  n_gamma <- n_giorni/365*24 + 6
  for(i in 25:n_gamma) {
      Gamma <- c(Gamma,-sum(Gamma[(i-23):(i - 1)]))
  }
  
  Gamma <- Gamma + rnorm(length(Gamma), sd=sd_stag)

  x <- floor(rep(seq(1,365, length.out = 24),n_anni) + rep(365*c(0:(n_anni-1)), each=24))
  x <- c(c(-44,-29,-14),x,c(1840,1855,1870))
  fit = loess(Gamma ~ x, span=0.065, degree=2)
  


  stag_average <- predict(fit, data.frame(x = 1:n_giorni))
  return(stag_average)
}

get_processi_STAG <- function(nProc, n,sd) {
  processi <- matrix(NA, nrow=nProc, ncol=n)
  as <- sample(2:22, nProc, replace=TRUE)
  
  for(i in 1:nProc) {
    processi[i,] <- processo_STAG(n, sd = sd,as[i])
  }
  return(processi)
}

get_processi_ARMA <- function(nProc, n, coefAR, coefMA,d,sd) {
  processi <- matrix(NA, nrow=nProc, ncol=n)
  for(i in 1:nProc) {
    processi[i,] <- noise_ARMA(n, coefAR,coefMA,d,sd)
  }
  return(processi)
}




lin.out.variabile <- function(input, processi, unstable_vars) {
  
  coef.main <- runif(ncol(input) + 1, min = -5, max = 5)
  coef.int <- runif(length(unstable_vars), min = -0.75, max = 0.75)
  
  coefficienti <- matrix(rep(coef.main[2:(ncol(input)+1)], each=n_giorni), nrow=ncol(input), ncol=n_giorni, byrow = TRUE)
  
    
  for (j in 1:nrow(processi)) {
    processi[j,] <- processi[j,] * coef.int[j]
  }
  
  coefficienti[unstable_vars,] <- coefficienti[unstable_vars,] + processi
  
  output <- rep(NA, n_giorni*daily_obs)
  
  for (i in 1:n_giorni) {
      output[((i-1)*daily_obs + 1):(i*daily_obs)] <- as.matrix(input[((i-1)*daily_obs + 1):(i*daily_obs),]) %*% as.matrix(coefficienti[,i])
  }

  return(list(output = output, coefficienti = coefficienti,interazioni = coef.int))
}


coeff_plot <- function(coefficienti) {
  
  #coefficienti <- output$coefficienti
  coeff.df <- as.data.frame(t(coefficienti))
  coeff.df$Days <- 1:n_giorni
  
  coeff.for.plot <- coeff.df[,c("V1","Days")]
  colnames(coeff.for.plot) <- c("Coeff","Days")
  coeff.for.plot$Var <- "V1"
  
  if(nvar_input >=2) {
  for ( i in 2:(ncol(coeff.df)-1) ) {
    tmp <- cbind(coeff.df[,c(i,ncol(coeff.df))],colnames(coeff.df)[i])
    colnames(tmp) <- c("Coeff","Days","Var")
    coeff.for.plot <- rbind(coeff.for.plot, tmp)
  }
  }
  
  coeff.for.plot$Var <- factor(coeff.for.plot$Var, levels = colnames(coeff.df)[-ncol(coeff.df)])
  
  plot <- ggplot(coeff.for.plot, aes(x = Days, y=Coeff, color = Var)) + geom_line() + labs(title="Coefficienti simulati - andamento giornaliero", x="giorni",y ="coefficienti") + theme_bw() + theme(panel.border = element_rect(color = "black", linewidth = 0.3, fill = NA))
  
  return(list(plot=plot))
}


```

```{r Simulazione - ANDAMENTI NEGLI EFFETTI DEI COEFFICIENTI}
sample(1000:10000, 1)


# PRIMO DATASET - Interazione con effetto ARMA latente.
nvar_input <- 80
n_giorni <- 1825
n_anni <- 5
daily_obs <- 10
sd_WN <- sqrt(80)
sd_noiseARMA <- sqrt(0.1)
sd_noiseARMA <- sqrt(1)
mu <- rep(0,nvar_input)

set.seed(7014)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

unstable_vars <- sample(1:nvar_input,nvar_input*1)
processi <- get_processi_ARMA(length(unstable_vars),n_giorni, coefAR = c(0.6,0.3), coefMA = c(), d=0, sd_noiseARMA)

plot(processi[1,], type="l")
plot(processi[2,], type="l")
output <- lin.out.variabile(input, processi, unstable_vars)
output$interazioni
coeff.plot <- coeff_plot(output$coefficienti)$plot
coeff.plot

## Combino l'output con il noise.
noise <- noise.WN(n_giorni*daily_obs)
output.final <- output$output + noise
data <- create.data(output.final, input)
output_plot(data)

## Aggiungiamo al dataset le variabili per catturate questa cosa.
## Magari il giorno dell'anno.
data.dm <- get_daily_means(data)
data.dm$cor.l1   ## Non c'è correlazione ai ritardi.
data.dm$cor.l2  ## 

data$day_of_year <- rep(rep(1:365,each=daily_obs),5) ## La stagionalità ha l'ordine corretto ora.
data$day_of_year2 <- data$day_of_year^2

# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- rep(m1, each =daily_obs)
data$month_of_year <- factor(data$month_of_year)

all.plot(processi[1,], noise, output$output)
all.plot(processi[1,], noise, output.final)



# SECONDO DATASET - Interazione con effetto STAGIONALE latente.
nvar_input <- 80
n_giorni <- 1825
n_anni = 5
daily_obs <- 10
#daily_obs <- 40
sd_WN <- sqrt(80)
sd_stag <- sqrt(0.12)
mu <- rep(0,nvar_input)

set.seed(3202)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

unstable_vars <- sample(1:nvar_input,nvar_input*0.5)
processi <- get_processi_STAG(length(unstable_vars),n_giorni, sd_stag)

plot(processi[1,], type="l")
plot(processi[2,], type="l")

output <- lin.out.variabile(input, processi, unstable_vars)
output$interazioni
coeff.plot <- coeff_plot(output$coefficienti)$plot
coeff.plot

## Combino l'output con il noise.
noise <- noise.WN(n_giorni*daily_obs)
output.final <- output$output + noise
data <- create.data(output.final, input)
output_plot(data)

data.dm <- get_daily_means(data)
data.dm$cor.l1   ## Non c'è correlazione ai ritardi.
data.dm$cor.l2  ## 

data$day_of_year <- rep(rep(1:365,each=daily_obs),5) ## La stagionalità ha l'ordine corretto ora.
data$day_of_year2 <- data$day_of_year^2

# Lunghezza dei mesi
m <- c(31,28,31,30,31,30,31,31,30,31,30,31)
m1 <- c()
for (i in 1:12) {
  m1 <- c(m1,rep(i,m[i]))
}

data$month_of_year <- rep(m1, each =daily_obs)
data$month_of_year <- factor(data$month_of_year)
```

```{r Idee e teorie per il concept drift}

## Devo simulare dei drift adesso.
## Data drift e concept drift:
#  - Data drift sono cambiamenti in p(X)
#  - Concept drift sono cambiamenti nel tempo di P(Y|X)

## Tipologie di drift:
#  - Sudden drift, cambiamento repentino nelle regole
#  - Gradual drift, un sistema di regole ne rimpiazza un altro in modo graduale.
#  - Incremental drift, un sistema di regole si trasforma, nel tempo, in un altro.
#  - Drift ricorrente, un sistema di regole eventualmente ritorna.

# Incremental drift: La regola segue un andamento graduale. Esempio: a_t = a_t-1 + i/(10^4.7)*logsig(a_t-1)
# Sudden shift è improvviso, gradual drift le regole hanno probabilità di apparire che varia nel tempo.
# Drift ricorrente è il più strano.

# Aspetti importanti: il fatto che il drift inizi nella fase di training è un aspetto rilevante. Altrimenti PER FORZA i modelli si rompono.

```

```{r FUNZIONI per DATA DRIFT}
noise.WN <- function(n) {
  rnorm(n = n, sd = sd_WN)
}

# Shock esponenziale giornaliero.
shock.exp <- function(n_giorni,a,b,c) {
  shock <- rep(0,n_giorni)
  shock[1:n_giorni >= a] <- 0 + c*exp(b*(a:n_giorni-a))
  shock
}

incremental.shift <- function(n_giorni, M, a, b) {
  shift <- rep(0,n_giorni)
  shift[1:n_giorni >= a] <- M*(1-exp(b*(a:n_giorni - a)))
  shift
}

gradual.shift <- function(n_giorni, length, a, b) {
  shift <- rep(0,n_giorni)
  p <- seq(0,1,length.out = length)
  levels <- c()
  for(prob in p) {
    levels <- c(levels,sample(c(0,b),1,replace=TRUE,prob = c(1-prob,prob)))
  }
  shift[a:(a+length - 1)] <- levels
  shift[(a+length):n_giorni] <- b
  shift
}

lin.out <- function(input) {
  ## coef[1] è l'intercetta, il resto i coefficienti.
  coef <- runif(ncol(input) + 1, min = -5, max = 5)
  output <- apply(input,1,function(x) coef[1] + t(as.matrix(x))%*%as.matrix(coef[2:length(coef)]))
  cat("\nMain effects: ",coef)
  return(output)
}

output.plot <- function(noiseWN, output, output.final) {
  par(mfrow=c(3,1))
  plot(1:length(noiseWN),noiseWN,ylab="", xlab="giorni", type="l")
  title("Termine d'errore")
  plot(1:length(output),output,ylab="", xlab="giorni", type="l")
  title("Combinazione lineare degli input")
  plot(1:length(output.final),output.final,ylab="", xlab="giorni", type="l")
  title("Output finale")
}
```

```{r Simulazioni - DATA DRIFT nelle NON OSSERVATE}
# Data drift nelle quantità osservate e non osservate:
# Iniziamo dal sudden drift nella componente non osservata.
sample(1000:10000,1)


## PRIMO DATASET - SUDDEN SHIFT NELLA COMPONENTE NON OSSERVATA - FUORI DAL TRAINING SET
nvar_input <- 20
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
shift_day <- 1700
shift_intensity <- 10

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

## Voglio generare l'errore in modo che abbia un balzo verso l'alto all'improvviso.
# Genero un processo con media 0 negli errori, che poi aumenta da un certo istante temporale in poi.
output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
obs_shifted <- (shift_day*daily_obs + 1):(n_giorni*daily_obs)
noiseWN[obs_shifted] <- noiseWN[obs_shifted] + shift_intensity
output.final <- output + noiseWN

output.plot(noiseWN,output,output.final)
data <- create.data(output.final,input,daily_obs)


## SECONDO DATASET - SUDDEN SHIFT NELLA COMPONENTE NON OSSERVATA con RIASSORBIMENTO - FUORI DAL TRAINING SET
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
# Giorno dello shift
# Formula impulso exp -> 1 + c*exp(b*(t-a))*I(t>a)
# a = giorno dello shift
# b = decadimento dello shift
# c = intensità dello shift
a <- 1200
c <- 12
b <- -0.008

# Usato lo stesso seed per confrontare le reazioni sullo stesso dataset
set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

## Voglio generare l'errore in modo che abbia un balzo verso l'alto all'improvviso.
# Genero un processo con media 0 negli errori, che poi aumenta da un certo istante temporale in poi.
output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
shockExp <- shock.exp(n_giorni,a,b,c)
plot(shockExp, type="l")
noiseWN_shocked <- noiseWN + rep(shockExp,each=daily_obs)
output.final <- output + noiseWN_shocked

output.plot(noiseWN_shocked,output,output.final)


## TERZO e QUARTO DATASET - Stessi shock, ma all'interno del training.
# Semplicemente si tratta di modificare, rispettivamente, shift_day ed a.




## QUINTO DATASET - INCREMENTAL DRIFT nella non osservata - FUORI DAL TRAINING SET
# Devo sommare un processo che varia nel tempo alla componente non osservata.
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
a <- 900
b <- -0.004
M <- 20  ## Il plateau.

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

## Voglio generare l'errore in modo che abbia un balzo verso l'alto all'improvviso.
# Genero un processo con media 0 negli errori, che poi aumenta da un certo istante temporale in poi.
output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
incremental_shift <- incremental.shift(n_giorni, M, a, b)
plot(incremental_shift, type="l")

noiseWN_shifted <- noiseWN + rep(incremental_shift, each=daily_obs)
output.final <- output + noiseWN_shifted
output.plot(noiseWN_shifted,output,output.final)

## SESTO DATASET - INCREMENTAL DRIFT nel TRAINING SET.


## SETTIMO DATASET - GRADUAL DRIFT fuori dal TRAINING SET.
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
a <- 900
b <- -20
length <- 500 # La durata dello shift.

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

output <- lin.out(input)
noiseWN <- noise.WN(n_giorni*daily_obs)
gradual_shift <- gradual.shift(n_giorni, length, a, b)
plot(gradual_shift, type="l")
noiseWN_shifted <- noiseWN + rep(gradual_shift, each=daily_obs)
output.final <- output + noiseWN_shifted
output.plot(noiseWN_shifted,output,output.final)


## OTTAVO DATASET - GRADUAL SHIFT nel TRAINING SET.

## Aspettiamo per il recurrent shift.

```

```{r FUNZIONI per DATA DRIFT nelle OSSERVATE}

noise.WN <- function(n) {
  rnorm(n = n, sd = sd_WN)
}

# Shock esponenziale giornaliero.
shock.exp <- function(n_giorni,a,b,c) {
  shock <- rep(0,n_giorni)
  shock[1:n_giorni >= a] <- 0 + c*exp(b*(a:n_giorni-a))
  shock
}

incremental.shift <- function(n_giorni, M, a, b) {
  shift <- rep(0,n_giorni)
  shift[1:n_giorni >= a] <- M*(1-exp(b*(a:n_giorni - a)))
  shift
}

gradual.shift <- function(n_giorni, length, a, b) {
  shift <- rep(0,n_giorni)
  p <- seq(0,1,length.out = length)
  levels <- c()
  for(prob in p) {
    levels <- c(levels,sample(c(0,b),1,replace=TRUE,prob = c(1-prob,prob)))
  }
  shift[a:(a+length - 1)] <- levels
  shift[(a+length):n_giorni] <- b
  shift
}


input_sudden_shift <- function(shifted_var,n_giorni, a, b) {
  
  shift <- rep(0,n_giorni)
  shift[a:n_giorni] <- b
  
  plot <- ggplot(data=data.frame(y = shift, days = 1:n_giorni), aes(x=days, y =y)) + geom_line(stat="identity")
  
  mu <- rep(0,nvar_input)
  input <- NULL
  for (j in 1:n_giorni) {
    mu[shifted_var] <- shift[j]
    input <- rbind(input,mvrnorm(n = daily_obs, mu = mu, Sigma = Sigma))
  }
  
  return(list(input=input, plot=plot, shift = shift))
}

input_sudden_shift_exp <- function(shifted_var,n_giorni, a, b, c) {
  
  shift <- shock.exp(n_giorni,a,b,c)
  
  plot <- ggplot(data=data.frame(y = shift, days = 1:n_giorni), aes(x=days, y =y)) + geom_line(stat="identity")
  
  mu <- rep(0,nvar_input)
  input <- NULL
  for (j in 1:n_giorni) {
    mu[shifted_var] <- shift[j]
    input <- rbind(input,mvrnorm(n = daily_obs, mu = mu, Sigma = Sigma))
  }
  
  return(list(input=input, plot=plot, shift = shift))
}

lin.out <- function(input) {
  ## coef[1] è l'intercetta, il resto i coefficienti.
  coef <- runif(ncol(input) + 1, min = -5, max = 5)
  output <- apply(input,1,function(x) coef[1] + t(as.matrix(x))%*%as.matrix(coef[2:length(coef)]))
  cat("\nMain effects: ",coef)
  return(output)
}

```

```{r Simulazioni - DATA DRIFT nelle OSSERVATE}

## PRIMO DATASET - SUDDEN SHIFT nelle MEDIE - non riassorbimento - FUORI DAL TRAINING.
nvar_input <- 20
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
a <- 1200
b <- 15

set.seed(1506)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
shifted_var <- sample(1:nvar_input,nvar_input*1)
input <- input_sudden_shift(shifted_var, n_giorni, a, b)
## Voglio generare l'errore in modo che abbia un balzo verso l'alto all'improvviso.
# Genero un processo con media 0 negli errori, che poi aumenta da un certo istante temporale in poi.
output <- lin.out(input$input)
noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output + noiseWN

output.plot(noiseWN,output,output.final)
data <- create.data(output.final,input$input)

## SECONDO DATASET - SUDDEN SHIFT nelle MEDIE - FUORI dal TRAINING
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
a <- 1200
c <- 12
b <- -0.004

set.seed(1506)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
shifted_var <- sample(1:nvar_input,nvar_input*1)
input <- input_sudden_shift_exp(shifted_var, n_giorni, a, b, c)
input$plot

## Voglio generare l'errore in modo che abbia un balzo verso l'alto all'improvviso.
# Genero un processo con media 0 negli errori, che poi aumenta da un certo istante temporale in poi.
output <- lin.out(input$input)
noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output + noiseWN

output.plot(noiseWN,output,output.final)


## TERZO e QUARTO DATASET - SUDDEN SHIFT nelle MEDIE - nel TRAINING
a <- 450


## QUINTO DATASET - INCREMENTAL DRIFT e GRADUAL DRIFT

```

```{r FUNZIONI per CONCEPT DRIFT}

create.data <- function(output, input, daily_obs) {
  data <- cbind(input,output,rep(1:n_giorni,each=daily_obs))
  colnames(data)[ncol(data)] <- "Days"
  return(data)
}

coeff_plot <- function(coefficienti) {
  
  #coefficienti <- output$coefficienti
  coeff.df <- as.data.frame(t(coefficienti))
  coeff.df$Days <- 1:n_giorni
  
  coeff.for.plot <- coeff.df[,c("V1","Days")]
  colnames(coeff.for.plot) <- c("Coeff","Days")
  coeff.for.plot$Var <- "V1"
  
  if(nvar_input >=2) {
  for ( i in 2:(ncol(coeff.df)-1) ) {
    tmp <- cbind(coeff.df[,c(i,ncol(coeff.df))],colnames(coeff.df)[i])
    colnames(tmp) <- c("Coeff","Days","Var")
    coeff.for.plot <- rbind(coeff.for.plot, tmp)
  }
  }
  
  coeff.for.plot$Var <- factor(coeff.for.plot$Var, levels = colnames(coeff.df)[-ncol(coeff.df)])
  
  plot <- ggplot(coeff.for.plot, aes(x = Days, y=Coeff, color = Var)) + geom_line() + labs(title="Coefficienti simulati - andamento giornaliero", x="giorni",y ="coefficienti") + theme_bw() + theme(panel.border = element_rect(color = "black", linewidth = 0.3, fill = NA))
  
  return(list(plot=plot))
}


lin.out.sudden.shift <- function(input, a) {
  # Prima regola
  coef.main1 <- runif(ncol(input) + 1, min = -5, max = 5)
  # Seconda regola
  coef.main2 <- runif(ncol(input), min = -5, max = 5)
  
  coefficienti <- matrix(rep(coef.main1[2:(ncol(input)+1)], each=n_giorni), nrow=ncol(input), ncol=n_giorni, byrow = TRUE)
  
  # Dal giorno a, i coefficienti cambiano.
  coefficienti[,a:n_giorni] <- matrix(rep(coef.main2, each = length(a:n_giorni)),byrow=TRUE,nrow=ncol(input), ncol=length(a:n_giorni))
  
  output <- rep(NA, n_giorni*daily_obs)
  
  for (i in 1:n_giorni) {
      output[((i-1)*daily_obs + 1):(i*daily_obs)] <- as.matrix(input[((i-1)*daily_obs + 1):(i*daily_obs),]) %*% as.matrix(coefficienti[,i])
  }

  return(list(output = output, coefficienti = coefficienti))
  
}

lin.out.incremental.shift <- function(input, a, duration) {
  
  # Prima regola
  coef.main1 <- runif(ncol(input) + 1, min = -5, max = 5)
  # Seconda regola
  coef.main2 <- runif(ncol(input), min = -5, max = 5)
  
  coefficienti <- matrix(rep(coef.main1[2:(ncol(input)+1)], each=n_giorni), nrow=ncol(input), ncol=n_giorni, byrow = TRUE)
  
  # Devo preparare un percorso che mi porta al secondo coefficiente.
  coefficienti[,a:(a + duration - 1)] <- t(apply(cbind(coef.main1[-1],coef.main2),1,function(x) seq(x[1],x[2],length.out=duration)))
  
  coefficienti[,(a + duration):n_giorni] <- matrix(rep(coef.main2, each=length((a + duration):n_giorni)), nrow=ncol(input), ncol=length((a + duration):n_giorni), byrow = TRUE)
  
  output <- rep(NA, n_giorni*daily_obs)
  
  for (i in 1:n_giorni) {
      output[((i-1)*daily_obs + 1):(i*daily_obs)] <- as.matrix(input[((i-1)*daily_obs + 1):(i*daily_obs),]) %*% as.matrix(coefficienti[,i])
  }

  return(list(output = output, coefficienti = coefficienti))
}


lin.out.gradual.shift <- function(input, a, duration) {
  
  # Prima regola
  coef.main1 <- runif(ncol(input) + 1, min = -5, max = 5)
  # Seconda regola
  coef.main2 <- runif(ncol(input), min = -5, max = 5)
  
  coefficienti <- matrix(rep(coef.main1[2:(ncol(input)+1)], each=n_giorni), nrow=ncol(input), ncol=n_giorni, byrow = TRUE)
  
  # Devo preparare un percorso che mi porta al secondo coefficiente.
  coefficienti[,a:(a + duration - 1)] <- t(apply(cbind(coef.main1[-1],coef.main2),1,function(x) seq(x[1],x[2],length.out=duration)))
  
  coefficienti[,(a + duration):n_giorni] <- matrix(rep(coef.main2, each=length((a + duration):n_giorni)), nrow=ncol(input), ncol=length((a + duration):n_giorni), byrow = TRUE)
  
  output <- rep(NA, n_giorni*daily_obs)
  
  for (i in 1:n_giorni) {
      output[((i-1)*daily_obs + 1):(i*daily_obs)] <- as.matrix(input[((i-1)*daily_obs + 1):(i*daily_obs),]) %*% as.matrix(coefficienti[,i])
  }

  return(list(output = output, coefficienti = coefficienti))
}


```

```{r CONCEPT DRIFT}

# Cambio delle regole p(Y|X).

## PRIMO DATASET - SUDDEN DRIFT.
nvar_input <- 2
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
a <- 1200

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

output <- lin.out.sudden.shift(input, a)
plot <- coeff_plot(output$coefficienti)
plot

noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output$output + noiseWN

output.plot(noiseWN,output$output,output.final)
data <- create.data(output.final,input,daily_obs)


## SECONDO DATASET - INCREMENTAL SHIFT.
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
a <- 600
duration <- 800

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

output <- lin.out.incremental.shift(input, a, duration)
plot <- coeff_plot(output$coefficienti)
plot

noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output$output + noiseWN

output.plot(noiseWN,output$output,output.final)
data <- create.data(output.final,input,daily_obs)


## TERZO DATASET - GRADUAL SHIFT.
nvar_input <- 5
n_giorni <- 1825
daily_obs <- 10
sd_WN <- sqrt(10)
mu <- rep(0,nvar_input)
#Giorno dello shift
a <- 600
duration <- 800

set.seed(9857)
Sigma <- get_Sigma(nvar_input, rho=0.15)$mat
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))

output <- lin.out.gradual.shift(input, a, duration)
plot <- coeff_plot(output$coefficienti)
plot

noiseWN <- noise.WN(n_giorni*daily_obs)
output.final <- output$output + noiseWN

output.plot(noiseWN,output$output,output.final)
data <- create.data(output.final,input,daily_obs)


```



```{r Prove fixing rf}
nvar_input <- 80
n_giorni <- 1825
daily_obs <- 10
sd_WN <- 150
mu <- rep(0,nvar_input)
Sigma <- diag(rep(1,nvar_input))

set.seed(4921)
input <- as.data.frame(mvrnorm(n = n_giorni*daily_obs,mu = mu, Sigma = Sigma))
output.lin <- lin.out(input)
data <- create.data(output.lin,input, daily_obs)

## Ridge R2 -> 

```




```{r Plot delle relazioni quadratiche}
## V1 e output
plot.V1 <- ggplot(data, aes(x = V1, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V1

plot.V2 <- ggplot(data, aes(x = V2, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V2

plot.V3 <- ggplot(data, aes(x = V3, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V3

plot.V4 <- ggplot(data, aes(x = V4, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V4

plot.V5 <- ggplot(data, aes(x = V5, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V5

plot.V6 <- ggplot(data, aes(x = V6, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V6

plot.V7 <- ggplot(data, aes(x = V7, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V7

plot.V8 <- ggplot(data, aes(x = V8, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V8

plot.V9 <- ggplot(data, aes(x = V9, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V9

plot.V10 <- ggplot(data, aes(x = V10, y = output)) + geom_point(stat ="identity") + geom_smooth()
plot.V10

data.quad
```

```{r Parametri degenerazione}
## Parametri degenerazione
nsets <- 4
retrain <- 60  ## Multiplo di 3

max_t0 <- 365
finestra <- 30
mse_window = 60

## 1006 giorni per studiare la degenerazione.
max_dt <- max(data$Days) - (364 + max_t0) - finestra - mse_window

## Giorni ai quali stimare i modelli.
days_selected <- seq(365,365+max_t0, by = 3)
```

```{r Standardizzazione se necessaria}
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

quant <- c()
for(var in X) {
  if(is.numeric(data[,var])) quant <- c(quant,var)
}
qual <- setdiff(X,quant)
data[,quant] <- scale(data[,quant])

```

```{r Ottenimento dei SSE}

MSSE_t0 <- function(data_x, days_selected, output_pos, days_pos)  {
  
  MSSE_df <- data.frame(MSSE_t0 = NA)
  
  for (i in days_selected) {
    
    t0_train <- (data_x[,"Days"] > i & data_x[,"Days"] <= (i + mse_window))
    MSSE_df[i - 364,] <- mean((data_x[t0_train,"output"] - mean(data_x[t0_train,"output"]))^2)

  }
  
  return(MSSE_df)
}

data_x <- na.omit(data)
MSSE_days_sel <- na.omit(MSSE_t0(data_x[,c("Days","output")], days_selected))

```


## Ridge
```{r CV Ridge}

ridge_estimates <- function(training, train_set, test_set, lambda) {
  
  tmp_ridge <- glmnet(x = training[train_set,-c(days.pos,out.pos)], y = training[train_set,out.pos],alpha = 0, lambda = lambda)
    
  pred_ridge = predict(tmp_ridge, newx=training[test_set,-c(days.pos,out.pos)])
  errori_ridge = apply((training[test_set, out.pos] - pred_ridge)^2,2,mean)
    
  return(errori_ridge)
}

cv_ridge <- function(training, nsets, lambda, time_wise) {
  
  if(time_wise == FALSE) {
   order <- sample(1:nrow(training),nrow(training)) 
  }
  
  errori_ridge <- matrix(NA,nrow=nsets, ncol=length(lambda))
  
  #Inizia il ciclo per ogni subset.
  for (i in 1:nsets) {
    #i = 5
    if(time_wise == TRUE) {
      
      dim_subset = floor(nrow(training)/(nsets+1))
      train_set <- 1:(i*dim_subset)
      test_set <- (i*dim_subset + 1):((i+1)*dim_subset)  ## Blocco successivo.
      
    }
    if(time_wise == FALSE) {
      
      dim_subset = floor(nrow(training)/nsets)
      test_set <- ((i-1)*dim_subset + 1):(i*dim_subset)
      train_set <- order[-test_set]
      test_set <- order[test_set]
    }
    
    cat("\n",i,": ")
    errori_ridge[i,] <- ridge_estimates(training, train_set, test_set, lambda)
  }

  return(errori_ridge)
}

```

```{r Degenerazione del modello ridge - Oggetti}
## Test degenerazione.
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Se input correlati agli output
#X <- setdiff(X, sample(X,5))

## Valori della degenerazione.
results <- matrix(NA,nrow = max_t0 + 1, ncol = max_dt)
colnames(results) <- paste0("dt: ",seq(1:max_dt))
rownames(results) <- seq(0:(max_t0))

##Caratteristiche dei modelli
models.attr <- matrix(NA,nrow = max_t0 + 1, ncol = 3)
rownames(models.attr) <- seq(0:(max_t0))
colnames(models.attr) <- c("R2","Lambda use","MSE a t0")

```

```{r Degenerazione del modello ridge - Ciclo}
data_x <- data[,c(X,"Days","output")]
data_x <- na.omit(data_x)
data_x <- model.matrix(~.,data_x)[,-1]

days.pos <- ncol(data_x) - 1
out.pos <- ncol(data_x)

## Griglia di Lambda da usare
lambda = 10^seq(2, -5, length=150)
## Per modello con interazione
#lambda = 10^seq(4, -1, length=200)
lambda.use <- NA

set.seed(20024)
for (i in days_selected) {
  ## Quali osservazioni per stimare il modello
  #i = 365
  train <- (data_x[,days.pos] >= (i - 364) & data_x[,days.pos] <= i)
  
  ## Quale parametro usare per la regolazione
  ## Se siamo in un multiplo di 15
  if(((i - 365) %% retrain) == 0) {
    
    m.ridge.cv <- cv_ridge(training = data_x[train,], nsets = nsets, lambda = lambda, time_wise = TRUE)
    lambda.use <- lambda[which.min(apply(m.ridge.cv,2,mean))]
  }
  
  m.ridge <- glmnet(x = data_x[train,-c(days.pos,out.pos)],y = data_x[train,out.pos], alpha = 0, lambda = lambda.use, family = "gaussian")
  
  
  ## MSE A T0 originale
   # t0_train <- (data_x[,days.pos] >= (i - finestra) & data_x[,days.pos] <= i)
   # fits.t0 <- predict(m.ridge, newx = data_x[t0_train,-c(days.pos,out.pos)])
   # mse.t0 <- mean((fits.t0 - data_x[t0_train,out.pos])^2)
  ## ABBIAMO IL MODELLO
  
  ## Mse a t0 con finestra successiva
   t0_train <- (data_x[,days.pos] > i & data_x[,days.pos] <= (i + mse_window))
   fits.t0 <- predict(m.ridge, newx = data_x[t0_train,-c(days.pos,out.pos)])
   mse.t0 <- mean((fits.t0 - data_x[t0_train,out.pos])^2)
  
  
  ## Salviamo i valori di lambda
  models.attr[i - 364,2] <- lambda.use
  models.attr[i - 364,3] <- mse.t0
   
   ## R2 di training
  fits <- predict(m.ridge, newx = data_x[train,-c(days.pos,out.pos)])
  SSE <- sum((fits - data_x[train,out.pos])^2)
  DEV <- sum((data_x[train,out.pos] - mean(data_x[train,out.pos]))^2)
  models.attr[i - 364,1] <- 1 - SSE/DEV
  
  ## Adesso le prestazioni.
  ## Previsioni del modello
  for_fits <- data_x[,days.pos] >= (i + mse_window + 1) & data_x[,days.pos] < (i + mse_window + max_dt + finestra)
  to_predict <- data_x[for_fits,out.pos]
  to_predict_X <- data_x[for_fits,-c(days.pos,out.pos)]
  
  ## Mi salvo anche i giorni.
  days_to_use <- data_x[for_fits,days.pos]
  
  ##Tutte le previsioni da i + 1 fino a i + 765, finestra poi di 30
  fits <- predict(m.ridge, newx = to_predict_X)
  
  ## Finestre mobili
  for (j in 1:max_dt) {
    #j = 1131
    ## Il vettore di previsioni ha una certa lunghezza.
    test <- (days_to_use >= (i + j + mse_window) & days_to_use < (i + j + mse_window + finestra))
    mse <- mean((to_predict[test] - fits[test])^2)
    
    results[i-364,j] <- mse
  }
  
  print(((i - 364)/(1 + max_t0))*100)
}

models.attr.ridge <- models.attr
results.ridge <- results
models.attr.ridge <- as.data.frame(na.omit(models.attr.ridge))
results.ridge <- as.data.frame(na.omit(results.ridge))

#plot(as.numeric(results.ridge[40,]))
models.attr.ridge <- as.data.frame(models.attr.ridge)
models.attr.ridge$R2_pred <- 1 - models.attr.ridge$`MSE a t0`/MSSE_days_sel$MSSE_t0

```

```{r Plot modello Ridge}

ridge_plot <- function(models.attr, results, title) {
  
  ## Dataframed
  res.for.plot <- as.data.frame(t(results))
  res.for.plot$dt <- as.numeric(gsub("dt: ","",rownames(res.for.plot)))
  rownames(res.for.plot) <- NULL
  
  ## Togliamo 
  res.scale.for.plot <- res.for.plot

  ## Dividiamo gli MSE assoluti per renderli relativi.
  for (i in 1:nrow(models.attr)) {
    res.scale.for.plot[,i] <- res.scale.for.plot[,i]/models.attr[i,3]   ## Dividiamo per MSE a t0
  }

  ## Calcoliamo i quantili
  qt <- matrix(NA, nrow = max_dt, ncol = 4)
  colnames(qt) <- c("q25","q50","q75","dt")
  rownames(qt) <- 1:max_dt

  ## Popoliamo la matrice con i quantili
  for (i in 1:max_dt) {
    qt[i,] <- c(quantile(as.numeric(res.scale.for.plot[i,-ncol(res.scale.for.plot)]), probs = c(0.25,0.5,0.75)),i)
  }
  qt.for.plot <- as.data.frame(qt)

  ## Proviamo a plottare.

  ## Abbiamo bisogno dei singoli punti.
  res.scale.for.plot1 <- res.scale.for.plot[,c(1,ncol(res.scale.for.plot))]
  colnames(res.scale.for.plot1) <- c("values","dt")


  for (i in 2:(ncol(res.scale.for.plot)-1)) {
    tmp.df <- as.matrix(res.scale.for.plot[,c(i,ncol(res.scale.for.plot))])
    colnames(tmp.df) <- c("values","dt")
    res.scale.for.plot1 <- rbind(res.scale.for.plot1,tmp.df)
  }
  
  plot <- ggplot(data = res.scale.for.plot1, aes(x = dt, y = values)) + geom_point(stat="identity", col = blu_points, size = 0.8) +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q25), color = "yellow") +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q50), color = "black") +
  geom_line( data=qt.for.plot, mapping = aes(x = dt, y = q75), color = "red") +
  ylab("MSE Relativo") + xlab("dT, days after trained") + ggtitle(title)
  
  return(plot)

}
```

```{r Plot degenerazione ridge}

## Facciamolo trasparente.
blu_points <- rgb(198,219,239, max = 255, alpha=60)

ridge.plot <- ridge_plot(models.attr.ridge, results.ridge, "Modello Ridge")
ridge.plot

```


## Random Forest
```{r CV Random Forest}
rf_estimates <- function(training, train_set, test_set, reg.grid) {
  
  numCores <- detectCores()
  cl <- makeCluster(numCores - 2)
  clusterExport(cl, c("X"))
  registerDoParallel(cl)
  
  r <- foreach (j = 1:nrow(reg.grid), .packages = "ranger") %dopar%  {
    tmp_rf <- ranger(output ~ .,
              data=training[train_set,],
              num.tree = reg.grid[j,1],
              importance = "impurity",
              mtry = reg.grid[j,2], oob.error=FALSE)
    
    test.fit <- mean((training[test_set,"output"] - predict(tmp_rf,training[test_set,X])$prediction)^2)
    
    list(errori = test.fit)
  }
  
  stopCluster(cl)
  
  errori <- rep(NA,nrow(reg.grid))
  
  for(i in 1:nrow(reg.grid)) {
    errori[i] <- r[[i]]$errori
  }
  
  return(errori)

}

cv.rf <- function(training, nsets, reg.grid, time_wise) {
  
  if(time_wise == FALSE) {
   order <- sample(1:nrow(training),nrow(training)) 
  }
  
  errori_rf <- matrix(NA,nrow=nsets, ncol=nrow(reg.grid))
  
  #Inizia il ciclo per ogni subset.
  for (i in 1:nsets) {
    
    if(time_wise == TRUE) {
      
      dim_subset = floor(nrow(training)/(nsets+1))
      train_set <- 1:(i*dim_subset)
      test_set <- (i*dim_subset + 1):((i+1)*dim_subset)  ## Blocco successivo.
      
    }
    if(time_wise == FALSE) {
      
      dim_subset = floor(nrow(training)/nsets)
      test_set <- ((i-1)*dim_subset + 1):(i*dim_subset)
      train_set <- order[-test_set]
      test_set <- order[test_set]
    }
    
    cat("\n",i,": ")
    errori_rf[i,] <- rf_estimates(training, train_set, test_set, reg.grid)
  }

  return(errori_rf)
}

```

```{r Prova random forest}

## Usando training-test split
anno1 <- data$Days <= 365
data_x <- na.omit(data[anno1,c(X,"output")])
cb1 <- sample(1:nrow(data_x), nrow(data_x)*0.8)
cb2 <- setdiff(1:nrow(data_x), cb1)
m.rf <- ranger(output ~ . ,
              data=data_x[cb1,c(X,"output")],
              num.tree = 300,
              importance = "impurity",
              mtry = 60, verbose =TRUE)
m.rf$r.squared  ## 0.80, molto vicino a quello che succede nel vero modello lineare.
## Aumentare il numero di osservazioni ha migliorato moltissimo la random forest.

# Proviamo a togliere le variabili poco influenti.
X1 <- names(sort(m.rf$variable.importance, decreasing =TRUE)[1:24])
m.rf <- ranger(output ~ . ,
              data=data_x[cb1,c(X1,"output")],
              num.tree = 300,
              importance = "impurity",
              mtry = 15, verbose =TRUE)
m.rf$r.squared  ## è meglio, ma non molto (3% di R2 in più)

fits <- predict(m.rf, data_x[cb2,X])
sse_cb2 <- sum((fits$predictions - data_x[cb2,"output"])^2)
dev_cb2 <- sum((data_x[cb2,"output"] - mean(data_x[cb2,"output"]))^2)
R2 <- 1 - sse_cb2/dev_cb2  ## 0.92 di R2 out-of-sample
R2
## Usando training-test temporali.
data_x <- na.omit(data[data$Days <= 400,])
cb1 <- data_x$Days <= 365
cb2 <- data_x$Days > 365

m.rf.temp <- ranger(output ~ . ,
              data=data_x[cb1,c(X,"output")],
              num.tree = 300,
              importance = "impurity",
              mtry = 60, verbose =TRUE)

fits <- predict(m.rf.temp, data_x[cb2,X])
sse_cb2 <- sum((fits$predictions - data_x[cb2,"output"])^2)
dev_cb2 <- sum((data_x[cb2,"output"] - mean(data_x[cb2,"output"]))^2)
R2 <- 1 - sse_cb2/dev_cb2  ## 0.87, é sensato.
R2


## Più dati aiutano la random forest, ma non risolvono il problema.
models.attr.ridge[1:10,]
```

```{r Degenerazione del modello RF - Oggetti}

## DATI
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Se input correlati agli output
#X <- setdiff(X, sample(X,5))

data_x <- data[,c(X,"Days","output")]
#training = data_x[anno1,]
#colnames(data_x)
data_x <- na.omit(data_x)

days.pos <- ncol(data_x) - 1
out.pos <- ncol(data_x)


## Valori della degenerazione.
results <- matrix(NA,nrow = max_t0 + 1, ncol = max_dt)
colnames(results) <- paste0("dt: ",seq(1:max_dt))
rownames(results) <- seq(0:(max_t0))

##Caratteristiche dei modelli
models.attr <- matrix(NA,nrow = max_t0 + 1, ncol = 4)
rownames(models.attr) <- seq(0:(max_t0))
colnames(models.attr) <- c("R2","NumTrees","Mtry","MSE a t0")


## Griglia
mtry <- seq(31,61, by =6)
num.trees <- seq(100,340, by = 60)
reg.grid <- expand.grid(num.trees,mtry)
colnames(reg.grid) <- c("Num trees","mtry")

```

```{r Degenerazione Random Forest - Regolazione}

regulated <- c()
mse <- list()
#max_t0 <- 45

k = 0

system.time(for (i in 365:(365+max_t0)) {
  
  if((i - 365) %% retrain == 0) {
    
    k = k + 1
    cat("\nRegolazione numero:",k)
    
    data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
    
    output <- cv.rf(data_x[data_to_use,-days.pos], nsets, reg.grid,time_wise = TRUE)
    
    # Salvo gli alberi.
    mse[[k]] <- output
    
    mse_medi <- apply(output,2,mean)
    
    ##Elemento ottimale della griglia.
    opt <- which.min(mse_medi)
    regulated <- rbind(regulated, c(i,opt, as.numeric(reg.grid[opt,])))

    
  }
})

colnames(regulated) <- c("Numero","Optimal","Trees","Mtry")
regulated.rf <- regulated

```

```{r Degenerazione del Random Forest - Ciclo}
#load("C:/Users/User/Documents/AI AGING/SIMULAZIONI/STAZIONARIETA/piu_INTERAZIONE/regulated.rf")
regulated <- regulated.rf

mtry_use <- NA
numTrees_use <- NA

## Riduco il numero di modelli da stimare ad un terzo.
days_selected <- seq(365,365+max_t0, by = 3)
numCores <- detectCores()
cl <- makeCluster(numCores - 1)
registerDoParallel(cl)
set.seed(576)
r <- foreach (i = days_selected, .packages = "ranger") %dopar% {
  #
  
  ## Dati da usare
  data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
  
  ## Aggiorniamo la struttura.
  reg_to_use <- sum(i >= regulated[,1])
  opt <- regulated[reg_to_use,2]
  
  m.rf <- ranger(output ~ . ,
              data=data_x[data_to_use,c(X,"output")],
              num.tree = reg.grid[opt,1],
              importance = "impurity",
              mtry = reg.grid[opt,2], oob.error=FALSE)
  
  ## R2.
  fits.rf <- predict(m.rf,data_x[data_to_use,X])$prediction
  SSE <- sum((data_x[data_to_use,out.pos] - fits.rf)^2)
  DEV <- sum((data_x[data_to_use,out.pos] - mean(data_x[data_to_use,out.pos]))^2)

  R2 <- 1 - SSE/DEV
  
  ## MSE a t0 mse_window.
  t0_train <- (data_x[,days.pos] > i & data_x[,days.pos] <= (i + mse_window))
  fits.t0 <- predict(m.rf, data_x[t0_train,-c(days.pos,out.pos)])$prediction
  mse.t0 <- mean((fits.t0 - data_x[t0_train,out.pos])^2)
  
  ## Previsioni del modello
  for_fits <- data_x[,days.pos] >= (i + mse_window + 1) & data_x[,days.pos] < (i + mse_window + max_dt + finestra)
  to_predict <- data_x[for_fits,out.pos]
  to_predict_X <- data_x[for_fits,-c(days.pos,out.pos)]
  
  ## Mi salvo anche i giorni.
  days_to_use <- data_x[for_fits,days.pos]
  
  ##Tutte le previsioni da i + 1 fino a i + max_dt, finestra poi di finestra
  fits <- predict(m.rf, to_predict_X)$prediction
  
  prestazioni <- rep(NA,max_dt)
  for (j in 1:max_dt) {

    test <- (days_to_use >= (i + j + mse_window) & days_to_use < (i + j + mse_window + finestra))
    mse <- mean((to_predict[test] - fits[test])^2)
    
    prestazioni[j] <- mse
  }
  
  list(R2 = R2, optimal = opt, mse_t0 = mse.t0, prestazioni = prestazioni, mtry = reg.grid[opt,2], numTrees =reg.grid[opt,1])
}
stopCluster(cl)

r[[1]]
## 

for (j in 1:length(days_selected)) {
  models.attr[days_selected[j] - 364,"R2"] <- r[[j]]$R2
  models.attr[days_selected[j] - 364,"NumTrees"] <- r[[j]]$numTrees
  models.attr[days_selected[j] - 364,"MSE a t0"] <- r[[j]]$mse_t0
  models.attr[days_selected[j] - 364,"Mtry"] <- r[[j]]$mtry
  results[days_selected[j] - 364,] <- r[[j]]$prestazioni
}

models.attr.rf <- models.attr
results.rf <- results
models.attr.rf <- as.data.frame(na.omit(models.attr.rf))
results.rf <- as.data.frame(na.omit(results.rf))
#models.attr

models.attr.rf <- as.data.frame(models.attr.rf)
models.attr.rf$R2_pred <- 1 - models.attr.rf$`MSE a t0`/MSSE_days_sel$MSSE_t0
```

```{r Plot modello RF}
rf_plot <- function(models.attr, results, title) {
  
  ## Dataframed
  res.for.plot <- as.data.frame(t(results))
  res.for.plot$dt <- as.numeric(gsub("dt: ","",rownames(res.for.plot)))
  rownames(res.for.plot) <- NULL
  
  ## Togliamo 
  res.scale.for.plot <- res.for.plot

  ## Dividiamo gli MSE assoluti per renderli relativi.
  for (i in 1:nrow(models.attr)) {
    res.scale.for.plot[,i] <- res.scale.for.plot[,i]/models.attr[i,4]   ## Dividiamo per MSE a t0
  }

  ## Calcoliamo i quantili
  qt <- matrix(NA, nrow = max_dt, ncol = 4)
  colnames(qt) <- c("q25","q50","q75","dt")
  rownames(qt) <- 1:max_dt

  ## Popoliamo la matrice con i quantili
  for (i in 1:max_dt) {
    qt[i,] <- c(quantile(as.numeric(res.scale.for.plot[i,-ncol(res.scale.for.plot)]), probs = c(0.25,0.5,0.75)),i)
  }
  qt.for.plot <- as.data.frame(qt)

  ## Proviamo a plottare.

  ## Abbiamo bisogno dei singoli punti.
  res.scale.for.plot1 <- res.scale.for.plot[,c(1,ncol(res.scale.for.plot))]
  colnames(res.scale.for.plot1) <- c("values","dt")


  for (i in 2:(ncol(res.scale.for.plot)-1)) {
    tmp.df <- as.matrix(res.scale.for.plot[,c(i,ncol(res.scale.for.plot))])
    colnames(tmp.df) <- c("values","dt")
    res.scale.for.plot1 <- rbind(res.scale.for.plot1,tmp.df)
  }
  
  plot <- ggplot(data = res.scale.for.plot1, aes(x = dt, y = values)) + geom_point(stat="identity", col = blu_points, size = 0.8) +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q25), color = "yellow") +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q50), color = "black") +
  geom_line( data=qt.for.plot, mapping = aes(x = dt, y = q75), color = "red") +
  ylab("MSE Relativo") + xlab("dT, days after trained") + ggtitle(title)
  
  return(plot)

}

```

```{r Plot degenerazione Random Forest}
blu_points <- rgb(198,219,239, max = 255, alpha=20)

rf.plot <- rf_plot(models.attr.rf, results.rf, "Modello RF - Input incorrelati")
rf.plot + ylim(0.7,1.25)

```



## Gradient boosting
```{r CV Gradient Boosting}

  tree.num <- function(shrink, depth) {
    
    trees = 3000
    #trees = 6000
    ## Se interazione 8000
    #trees = 12000
    
    if(depth == 1) trees = trees*2.5
    if(depth == 2) trees = trees*1.8
    if(depth ==3) trees = trees*1.5
    if(depth>=4) trees = trees*1.2
    
    return(trees)
  }

gb_estimates_par <- function(training, train_set, test_set, reg.grid) {
  
  tree.num <- function(shrink, depth) {
    
    trees = 3000
    #trees = 6000
    ## Se interazione 8000
    #trees = 12000
    
    if(depth == 1) trees = trees*2.5
    if(depth == 2) trees = trees*1.8
    if(depth ==3) trees = trees*1.5
    if(depth>=4) trees = trees*1.2
    
    return(trees)
  }
  
  cl <- makeCluster(8)
  registerDoParallel(cl)
  
  r <- foreach (j = 1:nrow(reg.grid), .packages = "gbm") %dopar%  {
    
    num.tree <- tree.num(shrink = reg.grid[j,2], depth =reg.grid[j,1])
    
    tmp_gb <- gbm(output~., data = training[train_set,], distribution = "gaussian",
                  n.trees = num.tree,
                  interaction.depth = reg.grid[j,1], shrinkage = reg.grid[j,2] )
    
    test.fit <- predict(tmp_gb, newdata = training[test_set,-ncol(training)], n.trees=1:num.tree)
    
    mses <- apply(test.fit, 2, function (pred) mean((training[test_set,"output"] - pred)^2))
    
    list(errori = mses[which.min(mses)], alberi = round(which.min(mses)/num.tree,2))
  }
  
  stopCluster(cl)
  
  
  errori <- rep(NA,nrow(grid))
  alberi <- rep(NA,nrow(grid))
  for(i in 1:nrow(grid)) {
    errori[i] <- r[[i]]$errori
    alberi[i] <- r[[i]]$alberi
  }
  risultati_list <- list(Errori = errori, Num_Alberi = alberi)
  return(risultati_list)
}


cv_gb <- function(training, nsets, reg.grid, time_wise) {
  
  if(time_wise == FALSE) {
   order <- sample(1:nrow(training),nrow(training)) 
  }
  
  errori_gb <- matrix(NA,nrow=nsets, ncol=nrow(reg.grid))
  num_alberi <- matrix(NA,nrow=nsets, ncol=nrow(reg.grid))
  
  #Inizia il ciclo per ogni subset.
  for (i in 1:nsets) {
    
    if(time_wise == TRUE) {
      
      dim_subset = floor(nrow(training)/(nsets+1))
      train_set <- 1:(i*dim_subset)
      test_set <- (i*dim_subset + 1):((i+1)*dim_subset)  ## Blocco successivo.
      
    }
    if(time_wise == FALSE) {
      
      dim_subset = floor(nrow(training)/nsets)
      test_set <- ((i-1)*dim_subset + 1):(i*dim_subset)
      train_set <- order[-test_set]
      test_set <- order[test_set]
    }
    
    cat("\n",i,":\n")
    #estimates <- gb_estimates(training, train_set, test_set, reg.grid)
    estimates <- gb_estimates_par(training, train_set, test_set, reg.grid)
    errori_gb[i,] <- estimates$Errori
    num_alberi[i,] <- estimates$Num_Alberi
  }

  return(list("results" = errori_gb,"alberi" = num_alberi))
}

```

```{r Degenerazione del modello GB - Oggetti}
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Se input correlati agli output
#X <- setdiff(X, sample(X,5))

data_x <- data[,c(X,"Days","output")]
data_x <- na.omit(data_x)
#training = data_x[anno1,]
#colnames(data_x)

days.pos <- ncol(data_x) - 1
out.pos <- ncol(data_x)


##Valori della degenerazione
results <- matrix(NA,nrow = max_t0 + 1, ncol = max_dt)
colnames(results) <- paste0("dt: ",seq(1:(max_dt)))
rownames(results) <- seq(0:(max_t0))

##Caratteristiche dei modelli
models.attr <- matrix(NA,nrow = max_t0 + 1, ncol = 4)
rownames(models.attr) <- seq(0:(max_t0))
colnames(models.attr) <- c("R2","Grid_opt","MSE a t0","Alberi")

##Griglia
shrinkage <- c(0.08)
depths <- 1:4
grid <- expand.grid(depths,shrinkage)
colnames(grid) <- c("depth","shrink")

```

```{r Prove Gradient Boosting}
anno1 <- data$Days <= 365

## Non ha senso regolare lo shrinkage
system.time(gb.cv <- cv_gb(data_x[anno1,-days.pos], nsets, grid, time_wise =TRUE))
colnames(gb.cv$alberi) <- round(grid$shrink,3)
which.min(apply(gb.cv$results,2,mean))
grid
## 80 input, 4 core: 137
## 80 input, 8 core: 99
## 80 input, 11 core: 89
grid[which.min(apply(gb.cv$results,2,mean)),]

anno2 <- data$Days > 365 & data$Days <= 365*2

## Proviamo a vedere.
system.time(risultati <- gb_estimates(training = data_x[,-days.pos], anno1, anno2, grid))  ## 36 secondi
system.time(risultati2 <- gb_estimates_par(training = data_x[,-days.pos], anno1, anno2, grid)) ## 9.5 secondi
grid[which.min(risultati$Errori),]

grid[which.min(risultati$Errori),]
grid[which.min(risultati2_list$Errori),]

## Proviamo cv_gb
boost.cv <- cv_gb(data_x[anno1, -days.pos], nsets, grid, time_wise =TRUE)


## Come fa il modello GB?
i=365
data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
data_use <- data_x[data_to_use,]
train <- sample(1:nrow(data_use), floor(nrow(data_use)*0.85))
training <- data_use[train,-days.pos]
validation <- data_use[-train, -days.pos]

num.tree <- tree.num(shrink=0.056, depth=1)
m.gb <- gbm(output~., data = training, distribution = "gaussian",
                  n.trees = num.tree,
                  interaction.depth = 1, shrinkage = 0.56)

test.fit <- predict(m.gb, newdata = validation, n.trees=1:num.tree)
    
mses <- apply(test.fit, 2, function (pred) mean((validation[,"output"] - pred)^2))
mses[which.min(mses)]  ## 136 alberi abbiamo il minimo.
## 3820 di mse.

fits <- predict(m.gb, newdata = data_use[,X], n.trees=which.min(mses))
SSE <- sum((fits - data_use$output)^2)
DEV <- sum((mean(data_use$output) - data_use$output)^2)
R2 <- 1 - SSE/DEV
R2   ##0.51 di R2. Non performa molto bene.
    

```

```{r Regolazione Gradient Boosting}

regulated <- c()
trees <- c()
mse <- c()

#max_t0 <- 45

k = 0

for (i in 365:(365+max_t0)) {
  
  if((i - 365) %% retrain == 0) {
    
    k = k + 1
    cat("\nRegolazione numero:",k)
    
    data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
    
    output <- cv_gb(data_x[data_to_use,-days.pos], nsets, grid,time_wise = TRUE)
    
    # Salvo gli alberi.
    trees <- rbind(trees,output$alberi)
    mse <- rbind(mse,output$results)
    
    mse_medi <- apply(output$results,2,mean)
    
    ##Elemento ottimale della griglia.
    opt <- which.min(mse_medi)
    regulated <- rbind(regulated, c(i,opt, as.numeric(grid[opt,])))
    
    ## Salviamo i parametri del modello.
    
  }
}

colnames(regulated) <- c("Numero","Optimal","Depth","Shrink")
regulated.gb <- regulated


```

```{r Degenerazione del modello GB - Ciclo}

# load("C:/Users/User/Documents/AI AGING/SIMULAZIONI/STAZIONARIETA/INTERAZIONE_con_ERRORE/regulated.gb")
# regulated <- regulated.gb
regulated  ## Ottenuto da "Regolazione Gradient Boosting"
days_selected <- seq(365,365+max_t0, by = 3)

numCores <- detectCores()
cl <- makeCluster(numCores - 2)
registerDoParallel(cl)
set.seed(435)
system.time(
r <- foreach (i = days_selected, .packages = "gbm") %dopar% {
  #i = 365
  ## Dati da usare
  data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
  
  ## Aggiorniamo la struttura.
  reg_to_use <- sum(i >= regulated[,1])
  
  ## Creo i training e i test set.
  data_use <- data_x[data_to_use,]
  train <- sample(1:nrow(data_use), floor(nrow(data_use)*0.85))
  training <- data_use[train,-c(days.pos)]
  testing <- data_use[-train,-c(days.pos)]
 
  opt <- regulated[reg_to_use,2]
  num.tree <- tree.num(shrink = grid[opt,2], depth = grid[opt,1])
  m.gb <- gbm(output~., data = training, distribution = "gaussian",
                  n.trees = num.tree,
                  interaction.depth = grid[opt,1], shrinkage = grid[opt,2] )
  
  ## Errori sul testing, per trovare il modello migliore.
  test.fit <- predict(m.gb, newdata = testing, n.trees=1:num.tree)
    
  mses <- apply(test.fit, 2, function (pred) mean((testing[,"output"] - pred)^2))
  
  ##Numero di alberi ottimale.
  opt_tree <- which.min(mses)

  ## R2.
  fits.gb <- predict(m.gb,data_use[,-c(days.pos,out.pos)], n.trees=opt_tree)
  SSE <- sum((data_use[,out.pos] - fits.gb)^2)
  DEV <- sum((data_use[,out.pos] - mean(data_use[,out.pos]))^2)

  R2 <- 1 - SSE/DEV
  
  ##Elemento utilizzato.
  optimal_used <- regulated[reg_to_use,2]
  
  
  ## MSE a t0.
  # t0_train <- (hospital_X[,days.pos] >= (i - finestra) & hospital_X[,days.pos] <= i)
  # fits.t0 <- predict(m.gb, hospital_X[t0_train,-c(days.pos,Wait.pos)], n.trees=opt_tree)
  # mse.t0 <- mean((fits.t0 - hospital_X[t0_train,Wait.pos])^2)
  
  ## MSE a t0 mse_window.
  t0_train <- (data_x[,days.pos] > i & data_x[,days.pos] <= (i + mse_window))
  fits.t0 <- predict(m.gb, data_x[t0_train,-c(days.pos,out.pos)], n.trees=opt_tree)
  mse.t0 <- mean((fits.t0 - data_x[t0_train,out.pos])^2)
  
  ## SALVO l'MSE a t0.
  
  for_fits <- data_x[,days.pos] >= (i + mse_window + 1) & data_x[,days.pos] < (i + mse_window + max_dt + finestra)
  to_predict <- data_x[for_fits,out.pos]
  to_predict_X <- data_x[for_fits,-c(days.pos,out.pos)]
  
  ## Mi salvo anche i giorni.
  days_to_use <- data_x[for_fits,days.pos]
  
  ##Tutte le previsioni da i + 1 fino a i + max_dt, finestra poi di finestra
  fits <- predict(m.gb, to_predict_X, n.trees=opt_tree)
  
  ## Finestre mobili
  
  prestazioni <- rep(NA,max_dt)
  for (j in 1:max_dt) {

    test <- (days_to_use >= (i + j + mse_window) & days_to_use < (i + j + mse_window + finestra))
    mse <- mean((to_predict[test] - fits[test])^2)
    
    prestazioni[j] <- mse
  }
  
  list(R2 = R2, optimal = optimal_used,opt_tree = round(opt_tree/num.tree,2), mse_t0 = mse.t0, prestazioni = prestazioni)
  
}
)

stopCluster(cl)

for (j in 1:length(days_selected)) {
  models.attr[days_selected[j] - 364,"R2"] <- r[[j]]$R2
  models.attr[days_selected[j] - 364,"Grid_opt"] <- r[[j]]$optimal
  models.attr[days_selected[j] - 364,"MSE a t0"] <- r[[j]]$mse_t0
  models.attr[days_selected[j] - 364,"Alberi"] <- r[[j]]$opt_tree
  results[days_selected[j] - 364,] <- r[[j]]$prestazioni
}

models.attr.gb <- models.attr
results.gb <- results
models.attr.gb <- as.data.frame(na.omit(models.attr.gb))
results.gb <- as.data.frame(na.omit(results.gb))

plot(as.numeric(results.rf[1,]))
plot(as.numeric(results.rf[20,]))
plot(as.numeric(results.rf[40,]))
plot(as.numeric(results.rf[80,]))

models.attr.gb <- as.data.frame(models.attr.gb)
models.attr.gb$R2_pred <- 1 - models.attr.gb$`MSE a t0`/MSSE_days_sel$MSSE_t0
```

```{r Plot modello GB}
gb_plot <- function(models.attr, results, title) {
  
  ## Dataframed
  res.for.plot <- as.data.frame(t(results))
  res.for.plot$dt <- as.numeric(gsub("dt: ","",rownames(res.for.plot)))
  rownames(res.for.plot) <- NULL
  
  ## Togliamo 
  res.scale.for.plot <- res.for.plot

  ## Dividiamo gli MSE assoluti per renderli relativi.
  for (i in 1:nrow(models.attr)) {
    res.scale.for.plot[,i] <- res.scale.for.plot[,i]/models.attr[i,3]   ## Dividiamo per MSE a t0
  }

  ## Calcoliamo i quantili
  qt <- matrix(NA, nrow = max_dt, ncol = 4)
  colnames(qt) <- c("q25","q50","q75","dt")
  rownames(qt) <- 1:max_dt

  ## Popoliamo la matrice con i quantili
  for (i in 1:max_dt) {
    qt[i,] <- c(quantile(as.numeric(res.scale.for.plot[i,-ncol(res.scale.for.plot)]), probs = c(0.25,0.5,0.75)),i)
  }
  qt.for.plot <- as.data.frame(qt)

  ## Proviamo a plottare.

  ## Abbiamo bisogno dei singoli punti.
  res.scale.for.plot1 <- res.scale.for.plot[,c(1,ncol(res.scale.for.plot))]
  colnames(res.scale.for.plot1) <- c("values","dt")


  for (i in 2:(ncol(res.scale.for.plot)-1)) {
    tmp.df <- as.matrix(res.scale.for.plot[,c(i,ncol(res.scale.for.plot))])
    colnames(tmp.df) <- c("values","dt")
    res.scale.for.plot1 <- rbind(res.scale.for.plot1,tmp.df)
  }
  
  plot <- ggplot(data = res.scale.for.plot1, aes(x = dt, y = values)) + geom_point(stat="identity", col = blu_points, size = 0.8) +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q25), color = "yellow") +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q50), color = "black") +
  geom_line( data=qt.for.plot, mapping = aes(x = dt, y = q75), color = "red") +
  ylab("MSE Relativo") + xlab("dT, days after trained") + ggtitle(title)
  
  return(plot)

}
```

```{r Plot degenerazione Gradient Boosting}
blu_points <- rgb(198,219,239, max = 255, alpha=20)

gb.plot <- gb_plot(models.attr.gb, results.gb, "Modello GB - Input incorrelati")
gb.plot + ylim(0.75,1.4)

```



## Rete Neurale
```{r Funzioni rete neurale}

create_model_l2 <- function(layers, size, input, lambda, seed) {
  
  set.seed(seed)
  seeds <- runif(layers + 1, min = 1000, max = 10000)
  
  model <- keras_model_sequential()
  
  model %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = input,
              kernel_initializer = initializer_random_normal(seed = seeds[1]),
              kernel_regularizer = regularizer_l2(l = lambda))
  
  if (layers >= 2) {
    for (h in 2:layers) {
      model %>% layer_dense(name = paste0("Hidden",h),
                          units = size[h],
                          activation = "relu",
                   kernel_initializer = initializer_random_normal(seed = seeds[h]),
                   kernel_regularizer = regularizer_l2(l = lambda))
   }
  }
  
  model %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = seeds[layers + 1]))
  
  model %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")
  
  return(model)
}

create_model_l1 <- function(layers, size, input, lambda, seed) {
  
  set.seed(seed)
  seeds <- runif(layers + 1, min = 1000, max = 10000)
  
  model <- keras_model_sequential()
  
  model %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = input,
              kernel_initializer = initializer_random_normal(seed = seeds[1]),
              kernel_regularizer = regularizer_l1(l = lambda))
  
  if (layers >= 2) {
    for (h in 2:layers) {
      model %>% layer_dense(name = paste0("Hidden",h),
                          units = size[h],
                          activation = "relu",
                   kernel_initializer = initializer_random_normal(seed = seeds[h]),
                   kernel_regularizer = regularizer_l1(l = lambda))
   }
  }
  
  model %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = seeds[layers + 1]))
  
  model %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")
  
  return(model)
}


create_model_dropout <- function(layers, size, n_input, dropout, seed) {
  
  set.seed(seed)
  seeds <- runif(layers + 1, min = 1000, max = 10000)
  
  model <- keras_model_sequential()
  
  model %>% layer_dropout(dropout, input_shape = n_input)
  
  model %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = seeds[1])) %>%
    layer_dropout(dropout)
  
  if (layers >= 2) {
    for (h in 2:layers) {
      model %>% layer_dense(name = paste0("Hidden",h),
                          units = size[h],
                          activation = "relu",
                   kernel_initializer = initializer_random_normal(seed = seeds[h])) %>%
        layer_dropout(dropout)
   }
  }
  
  model %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = seeds[layers + 1]))
    
  model %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")
  
  return(model)
}

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)

```

```{r CV Rete Neurale}

nn_estimates_drop_par <- function(training, train_set, test_set, reg.grid, reps, seed) {
  
  cl <- makeCluster(8)
  clusterExport(cl, c("layers","size","X"))
  registerDoParallel(cl)

  r <- foreach (j = 1:nrow(reg.grid), .packages = c("keras","tensorflow"), .export= c("create_model_dropout")) %dopar%  {
    
    models <- epoche <- c(NA, reps)
    
    early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                min_delta = 0,
                                patience = 8,
                                verbose = 0)
    
    for(h in 1:reps) {
      
      modello <- create_model_dropout(layers = layers, size = size,length(X), dropout = reg.grid[j,1], seed)
      
      storia <- modello %>% fit(x = as.matrix(training[train_set,X]),
            y = training[train_set,"output"],
            epochs = 1000,
            verbose = 0,
            validation_data = list(as.matrix(training[test_set,X]),training[test_set,"output"]),
            callbacks = early_stop)
      
      models[h] <- storia$metrics$val_mean_squared_error[which.min(storia$metrics$val_mean_squared_error)]
      epoche[h] <- which.min(storia$metrics$val_mean_squared_error)
    }
    
    
    ## Serve la replicazione migliore.
    list(errore = models[which.min(models)], epoche = epoche[which.min(models)])
  }
  
  stopCluster(cl)
  
  
  errori <- epoche <- rep(NA,nrow(grid))
  
  for(i in 1:nrow(grid)) {
    errori[i] <- r[[i]]$errore
    epoche[i] <- r[[i]]$epoche
  }
  
  risultati_list <- list(Errori = errori, Epoche = epoche)
  return(risultati_list)
}


nn_estimates_l2_par <- function(training, train_set, test_set, reg.grid, reps, seed) {
  
  cl <- makeCluster(8)
  clusterExport(cl, c("layers","size","X"))
  registerDoParallel(cl)

  r <- foreach (j = 1:nrow(reg.grid), .packages = c("keras","tensorflow"), .export= c("create_model_l2")) %dopar%  {
    
    models <- epoche <- c(NA, reps)
    
    early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                min_delta = 0,
                                patience = 2,
                                verbose = 0)
    
    for(h in 1:reps) {
      
      modello <- create_model_l2(layers = layers, size = size, input = length(X), lambda = reg.grid[j,1], seed)
      
      storia <- modello %>% fit(x = as.matrix(training[train_set,X]),
            y = training[train_set,"output"],
            batch_size = ceiling(length(train_set)/2),
            epochs = 1000,
            verbose = 0,
            validation_data = list(as.matrix(training[test_set,X]),training[test_set,"output"]),
            callbacks = early_stop)
      
      models[h] <- storia$metrics$val_mean_squared_error[which.min(storia$metrics$val_mean_squared_error)]
      epoche[h] <- which.min(storia$metrics$val_mean_squared_error)
    }
    
    
    ## Serve la replicazione migliore.
    list(errore = models[which.min(models)], epoche = epoche[which.min(models)])
  }
  
  stopCluster(cl)
  
  
  errori <- epoche <- rep(NA,nrow(grid))
  
  for(i in 1:nrow(grid)) {
    errori[i] <- r[[i]]$errore
    epoche[i] <- r[[i]]$epoche
  }
  
  risultati_list <- list(Errori = errori, Epoche = epoche)
  return(risultati_list)
}



cv_nn <- function(training, nsets, reg.grid, time_wise, reps) {
  
  if(time_wise == FALSE) {
   order <- sample(1:nrow(training),nrow(training)) 
  }
  
  errori_nn <- matrix(NA,nrow=nsets, ncol=nrow(reg.grid))
  num_epoche <- matrix(NA,nrow=nsets, ncol=nrow(reg.grid))
  
  #Inizia il ciclo per ogni subset.
  for (i in 1:nsets) {
    
    if(time_wise == TRUE) {
      
      dim_subset = floor(nrow(training)/(nsets+1))
      train_set <- 1:(i*dim_subset)
      test_set <- (i*dim_subset + 1):((i+1)*dim_subset)  ## Blocco successivo.
      
    }
    if(time_wise == FALSE) {
      
      dim_subset = floor(nrow(training)/nsets)
      test_set <- ((i-1)*dim_subset + 1):(i*dim_subset)
      train_set <- order[-test_set]
      test_set <- order[test_set]
    }
    
    cat("\n",i,":\n")
    #estimates <- nn_estimates_par(training, train_set, test_set, reg.grid, reps, runif(1, min =1000, max=9999))
    estimates <- nn_estimates_drop_par(training, train_set, test_set, reg.grid, reps, runif(1, min =1000, max=9999))
    errori_nn[i,] <- estimates$Errori
    num_epoche[i,] <- estimates$Epoche
  }

  return(list("results" = errori_nn,"epoche" = num_epoche))
}

```

```{r Degenerazione Rete Neurale - Oggetti}

vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Se input correlati agli output
#X <- setdiff(X, sample(X,5))

data_x <- data[,c(X,"Days","output")]
data_x <- na.omit(data_x)
data_x <- cbind(model.matrix(~., data=data_x[,X])[,-1],data_x[,c("Days","output")])
data_x <- as.data.frame(data_x)
X <- setdiff(colnames(data_x),escluse)

days.pos <- ncol(data_x) - 1
out.pos <- ncol(data_x)

##Valori della degenerazione
results <- matrix(NA,nrow = max_t0 + 1, ncol = max_dt)
colnames(results) <- paste0("dt: ",seq(1:(max_dt)))
rownames(results) <- seq(0:(max_t0))

##Caratteristiche dei modelli
models.attr <- matrix(NA,nrow = max_t0 + 1, ncol = 4)
rownames(models.attr) <- seq(0:(max_t0))
colnames(models.attr) <- c("R2","Grid_opt","MSE a t0","Epoche")
```

```{r Prove Rete Neurale}

## Dati. Devono essere matrici
anno1 <- data$Days <= 365
anno2 <- data$Days > 365 & data$Days <= 365*2
train_x <- as.matrix(data_x[anno1,X])
train_y <- data_x[anno1,"output"]
val_x <- as.matrix(data_x[anno2,X])
val_y <- data_x[anno2,"output"]

##  Costruisco anno1 e anno2.
tensorflow::set_random_seed(42)

layers <- 3
size <- c(300,300,300)
## c(150,150,150) overfitta sul dataset con 80 input.

s <- floor(runif(1, min = 1000, max = 9999))
s
modello <- create_model_l1(layers = layers, size = size, input = length(X), lambda = 3.5, seed = s)
  
storia <- modello %>% fit(x = train_x,
            y = train_y,
            batch_size = nrow(train_x),
            epochs = 1000,
            verbose = 1,
            validation_data = list(val_x,val_y),
            callbacks = early_stop)

## L1 - c(300,300,300). Più lambda è grande, più aggiorna piano. Meglio così.
# lambda = 0 --> val.mse = 2960
# lambda = 0.1 --> val.mse = 2800/3000
# lambda = 1 --> 2823
# lambda = 2 --> 2810
# lambda = 3 --> 2685
# lambda = 3.5 --> 2667
# lambda = 4 --> 2680
# lambda = 4.5 --> 2692
# lambda = 5 --> 2715
# lambda = 6 --> 2733
# lambda = 8 --> 2787
# lambda = 10 --> 2840

## L2
lambda <- seq(0.5,20, length.out = 40)
lambda <- seq(0.02,8, length.out = 40)

stopCluster(cl)
cl <- makeCluster(6)
registerDoParallel(cl)

set.seed(6574)
seeds <- runif(length(lambda), min = 1000, max = 9999)

## Questo foreach non funziona.
system.time(r <- foreach (i = 1:length(lambda), .packages = c("keras","tensorflow")) %dopar%  {
  
  early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0.005,
                                      patience = 15,
                                      verbose = 0)

  modello <- create_model_l1(layers = layers, size = size, input = length(X), lambda = lambda[i], seed = seeds[i])
  storia <- modello %>% fit(x = train_x,
            y = train_y,
            batch_size = nrow(train_x),
            epochs = 1000,
            verbose = 0,
            validation_data = list(val_x,val_y),
            callbacks = early_stop)
  
  list(seed = seeds[i], storia = storia, val_mse = tail(storia$metrics$val_mean_squared_error,1))
})

## input 80, layers <- 3, size <- c(300,300,300), length(lambda) == 20.
#2 core: 95 t
#3 core: 78 t
#4 core: 67 t
#5 core: 60 t
#6 core: 59 t
#7 core: 56 t
#8 core: 56 t
## 56 t fermo.

mses <- c(NA, length(lambda))
for (j in 1:length(lambda)) {
  mses[j] <- r[[j]]$storia$metrics$val_mean_squared_error[which.min(r[[j]]$storia$metrics$val_mean_squared_error)]
}
lambda[which.min(mses)]

## L1 performa meglio di L2.




## DROPOUT
layers <- 3
size <- c(50,50,50)

s <- floor(runif(1, min = 1000, max = 9999))
s
modello <- create_model_dropout(layers = layers, size = size, input = length(X), dropout = 0.8, seed = s)
  
storia <- modello %>% fit(x = train_x,
            y = train_y,
            batch_size = ceiling(nrow(train_x)/2),
            epochs = 300,
            verbose = 1,
            validation_data = list(val_x,val_y),
            callbacks = early_stop)

storia <- modello %>% fit(x = train_x,
            y = train_y,
            batch_size = nrow(train_x),
            epochs = 150,
            verbose = 1,
            validation_data = list(val_x,val_y))

dim(val_x)
## seed: 3388
# Dropout 0.2 --> 2745
# Dropout 0.3 --> 2804
# Dropout 0.4 --> 2961
# Dropout 0.5 --> 2745

## Valutando l'R2.
fits <- predict(modello, as.matrix(data_x[anno1 |anno2,X]), verbose=0, batch_size = 32)
SSE <- sum((fits - data_x[anno1 |anno2,"output"])^2)
DEV <- sum((mean(data_x[anno1 |anno2,"output"]) - data_x[anno1 |anno2,"output"])^2)
R2 <- 1 - SSE/DEV
R2

## Devo cambiare metodo per stimare il modello con dropout.

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0.01,
                                      patience = 4,
                                      verbose = 0)


## early_stop 15 funziona bene per il metodo del dropout, per gli altri va bene 2,4.


## Regolazione.
drops <- seq(0.05,0.95, by = 0.05)
set.seed(6574)
seeds <- runif(length(drops), min = 1000, max = 9999)

stopCluster(cl)
cl <- makeCluster(6)
registerDoParallel(cl)

## Questo foreach non funziona.
system.time(r <- foreach (i = 1:length(drops), .packages = c("keras","tensorflow")) %dopar%  {
  
  early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0.005,
                                      patience = 15,
                                      verbose = 0)

  modello <- create_model_dropout(layers = layers, size = size, input = length(X), dropout = drops[i], seed = seeds[i])
  storia <- modello %>% fit(x = train_x,
            y = train_y,
            batch_size = nrow(train_x),
            epochs = 1000,
            verbose = 0,
            validation_data = list(val_x,val_y),
            callbacks = early_stop)
  
  list(seed = seeds[i], storia = storia, val_mse = tail(storia$metrics$val_mean_squared_error,1))
})

r[[1]]$storia$metrics$val_mean_squared_error[which.min(r[[1]]$storia$metrics$val_mean_squared_error)]
res <-rep(NA,length(drops))
for(j in 1:length(drops)) {
  res[j] <- r[[j]]$storia$metrics$val_mean_squared_error[which.min(r[[j]]$storia$metrics$val_mean_squared_error)]
}

drops[which.min(res)]  ## 0.6

## Il dropout fa meglio degli altri, ed è più veloce.




```

```{r Prove NN 1 layer - Effetti Lineari}

vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 1
size <- c(160)

## Creiamo i modelli che servono.
modello <- create_model_l2(layers, size, input = length(X), lambda = 0, seed = floor(runif(1,10,9999)))

modello <- keras_model_sequential()

modello %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = length(X),
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")

##

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 1,
                                      verbose = 0)

##Stimiamo il modello.
storia <- modello %>% fit(
  as.matrix(data[,X]),
  as.matrix(data[,"output"]),
  validation_split = 0.2,
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## Performance?
fits <- predict(modello, as.matrix(data[,X]))
fits[1:10]
data[,"output"][1:10]

## Non è male.
SSE <- sum((fits - data[,"output"])^2)
DEV <- sum((data[,"output"] - mean(data[,"output"]))^2)
1 - SSE/DEV
## 0.83 di R2. Ottimo.

## Adesso vediamo con uno split manuale di data, per vedere se il problema è quello.
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## Sembra sostanzialmente identico.
## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV  ## 0.83 sul training set

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV  ## 0.82 sul validation set. Mi va bene 


## EFFETTI LINEARI
# layers = 1, size = 10 --> Input correlati, Incorrelati, R2 elevato in training e validation.
# layers = 1, size = 20 --> Input incorrelata, il divario inizia ad aumentare, anche se di poco.(0.9,0.87).
# layers = 1, size = 40 --> Input incorrelati, (0.93,0.89).
# layers = 1, size = 80 --> Input incorrelati, (0.97,0.92).
# layers = 1, size = 160 --> Input incorrelati, (0.91, 0.87). Differenza di 0.5 sempre. Va bene.

## Con gli input correlati, in realtà, è un po' più stabile sembra.

```

```{r Prove NN 2-3 layer - Effetti Lineari}

vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 3
size <- c(100,100,100)

modello <- keras_model_sequential()

modello %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = length(X),
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Hidden2",
                      units = size[2],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Hidden3",
                      units = size[3],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))


modello %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")


early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)


## Adesso vediamo con uno split manuale di data, per vedere se il problema è quello.
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## Sembra sostanzialmente identico.
## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV  ## 0.83 sul training set

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV 


## Qualsiasi cosa faccia, il modello funziona bene.
```

```{r Prove NN 1 layer - Effetti Quadratici}
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 1
size <- c(40)

## Creiamo i modelli che servono.
modello <- keras_model_sequential()

modello %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = length(X),
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 1,
                                      verbose = 0)

## DATI
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV  ## 0.60 sul training set

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV  ## 0.54 sul validation set. Mi va bene.


## EFFETTI QUADRATICI
# layers = 1, size = 10 --> Input correlati, R2 (0.6-0.54).
# layers = 1, size = 20 --> Input correlati, R2 (0.8-0.77).
# layers = 1, size = 40 --> Input correlati, R2 (0.86-0.83).
# layers = 1, size = 60 --> Input correlati, R2 (0.87-0.83).
# layers = 1, size = 150 --> Input correlati, R2 (0.88-0.83).

## Input incorrelati fa un po' più fatica.
# layers = 1, size = 20 --> Input incorrelati, R2 (0.76-0.61).
# layers = 1, size = 60 --> Input incorrelati, R2 (0.94-0.79).
# layers = 1, size = 150 --> Input incorrelati, R2 (0.98-0.88).


```

```{r Prove NN 2-3 layer - Effetti Quadratici}
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 3
size <- c(60,60,60)

modello <- keras_model_sequential()

modello %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = length(X),
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Hidden2",
                      units = size[2],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Hidden3",
                      units = size[3],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))),
              kernel_regularizer = regularizer_l2(l = 0))

modello %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))


modello %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")


early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)


## Adesso vediamo con uno split manuale di data, per vedere se il problema è quello.
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## Sembra sostanzialmente identico.
## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV  ## 0.83 sul training set

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV 

## INPUT INCORRELATI
# Layer = 2, size =c(10,10) --> (0.59-0.47)
# Layer = 2, size =c(20,20) --> (0.79-0.57)
# Layer = 2, size =c(40,40) --> (0.88-0.70)
# Layer = 2, size =c(60,60) --> (0.91-0.75)
# Layer = 2, size =c(100,100) --> (0.98-0.89)

# Layer = 3, size =c(10,10,10) --> (0.62-0.45)
# Layer = 3, size =c(20,20,20) --> (0.76-0.57)
# Layer = 3, size =c(40,40,40) --> (0.89-0.67)
# Layer = 3, size =c(60,60,60) --> (0.89-0.85)
# Layer = 3, size =c(100,100,100) --> (0.97-0.84)

## INPUT CORRELATI
# Layer = 2, size =c(10,10) --> (0.72-0.65)
# Layer = 2, size =c(20,20) --> (0.76-0.73)
# Layer = 2, size =c(40,40) --> (0.88-0.82)
# Layer = 2, size =c(60,60) --> (0.91-0.86)
# Layer = 2, size =c(100,100) --> (0.91-0.84)

# Layer = 3, size =c(10,10,10) --> (0.66-0.57)
# Layer = 3, size =c(20,20,20) --> (0.77-0.72)
# Layer = 3, size =c(40,40,40) --> (0.87-0.82)
# Layer = 3, size =c(60,60,60) --> (0.95-0.82)
# Layer = 3, size =c(100,100,100) --> (0.92-0.86)

```

```{r Prove regolazione del modello - Dropout}

vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 3
size <- c(400,400,400)

dropout = 0

modello <- create_model_dropout(layers = layers, size = size, length(X), dropout, seed=runif(1,10,9999))

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)


## Adesso vediamo con uno split manuale di data, per vedere se il problema è quello.
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 150,
  callbacks = early_stop
)

## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV 

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV 

## size = c(20) --> dropout=0.2 (0.71,0.68) ... 0.6 (0.44,0.41)
## size = c(40,40,40) --> dropout=0.6 (0.42,0.41) ... 0.2 (0.66,0.60) ... 0.1 (0.74,0.69) 0.05 (0.77,0.72)

## size = c(80*3) ---> dropout 0:(0.88,0.81), 0.05:(0.86,0.78), 0.1:(0.84,0.79), 0.3:(0.77,0.71)

## size = c(140*3) --> 0:(0.91,0.85) 0.05:(0.9,0.83) 0.1(0.89,0.82) 0.2(0.85,0.8)
## size = c(200*3) --> 0:(0.98,0.93) 0.05:(0.91,0.86) 0.1(0.92,0.84) 0.2(0.85,0.8)

## size = c(400*3) --> 0:(0.94,0.88) 0.05:(0.97,0.90) 0.1(0.96,0.89) 0.2(0.92,0.85) 0.6(0.75,0.69)

## size = c(800*3) --> 0:(0.98,0.93) 0.05:(0.97,0.92) 0.1(0.96,0.89) 0.2(0.92,0.85) 0.6(0.75,0.69)


```

```{r PROVE}

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 15,
                                      verbose = 0)

anno1 <- data_x$Days <= 365
first_year <- data_x[anno1,]

cb1 <- sample(1:nrow(first_year), 0.8*nrow(first_year))
cb2 <- setdiff(1:nrow(first_year), cb1)

model <- create_model_dropout(layers = 3, size = c(100,100,100), n_input=length(X), dropout=0, seed=43221)

storia <- model %>% fit(x = as.matrix(first_year[cb1,X]),
          y = first_year[cb1,"output"],
          batch_size = ceiling(length(cb1)/2),
          epochs = 300,
          verbose = 1,
          validation_data = list(as.matrix(first_year[cb2,X]),first_year[cb2,"output"]),
          callbacks = early_stop)

## performance nel training set.
fits.nn <- predict(model, as.matrix(first_year[cb1,X]), batch_size = 32, verbose = 0)
SSE <- sum((as.matrix(first_year[cb1,"output"]) - fits.nn)^2)
DEV <- sum((first_year[cb1,"output"] - mean(first_year[cb1,"output"]))^2)
1-SSE/DEV  ## 0.83
##nel validation set.
fits.nn <- predict(model, as.matrix(first_year[cb2,X]), batch_size = 32, verbose = 0)
SSE <- sum((as.matrix(first_year[cb2,"output"]) - fits.nn)^2)
DEV <- sum((first_year[cb2,"output"] - mean(first_year[cb2,"output"]))^2)
1-SSE/DEV  ## 0.82 nel validation set.

## Fuori dal primo anno?
finestra <- data_x$Days >= 366 & data_x$Days <= (366 + mse_window)
window <- data_x[finestra,]
fits.nn <- predict(model, as.matrix(window[,X]), batch_size = 32, verbose = 0)
SSE <- sum((as.matrix(window[,"output"]) - fits.nn)^2)
DEV <- sum((window[,"output"] - mean(window[,"output"]))^2)
1-SSE/DEV #0.76, tutto sommato molto buono.
```

```{r Riducendo i dati?}
## Con tutti i dati il modello è molto stabile.
anno1 <- data$Days <= 365
data <- data[anno1,]

## Vediamo la selezione nel primo anno.
vars <- colnames(data)
escluse <- c("output","Days")
X <- setdiff(vars,escluse)

## Abbiamo i dati, splittiamoli.

layers = 3
size <- c(350,350,350)

## Creiamo i modelli che servono.
modello <- keras_model_sequential()

modello %>% layer_dense(name = "Hidden1",
                      units = size[1],
                      activation = "relu",
                      input_shape = length(X),
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% layer_dense(name = "Hidden2",
                      units = size[2],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% layer_dense(name = "Hidden3",
                      units = size[3],
                      activation = "relu",
              kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% layer_dense(name = "Output",
                        units = 1,
                        activation = "linear",
                        kernel_initializer = initializer_random_normal(seed = floor(runif(1,10,9999))))

modello %>% compile(loss = "mean_squared_error",
                  optimizer ="Adam",
                  metrics = "mean_squared_error")

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)

## DATI
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 120,
  callbacks = early_stop
)

## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV  ## 0.60 sul training set

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV

## CORRELAZIONE
## c(5) --> (0.38,0.21) drop 0.05: (0.41,0.38) drop 0.1: (0.42,0.31) drop =.15 (0.43) drop 0.2(0.39,0.26)
## c(10) --> (0.47,0.34)
## c(20) --> (0.43,0.26)
## c(40) --> (0.58,0.39)
## c(60) --> (0.70,0.38)
## c(80) --> (0.64, 0.43)
## c(120) --> (0.73, 0.43)

## c(10,10) --> (0.52,0.23)
## c(20,20) --> (0.69, 0.47)
## c(40,40) --> (0.78,0.39)
## c(60,60) --> (0.78,0.48)
## c(90,90) --> (0.89,0.56)
## c(120,120) --> (0.89,0.47)

## c(10,10,10) --> (0.44,0.22) drop 0.05: (0.51,0.37) drop 0.1: (0.53,0.39) drop:0.15: (0.34,0.21) drop0.2: (0.37,0.3) drop (0.42,0.36) più alto non risolve.
## c(20,20,20) --> (0.57, 0.42)
## c(40,40,40) --> (0.66,0.39)
## c(60,60,60) --> (0.78,0.36)
## c(90,90,90) --> (0.89,0.46)
## c(120,120,120) --> (0.94,0.5)

## con DROPOUT
layers = 1
size <- c(10)

dropout = 0.6

modello <- create_model_dropout(layers = layers, size = size, input = length(X), dropout, seed=runif(1,10,9999))

early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                      min_delta = 0,
                                      patience = 2,
                                      verbose = 0)


## Adesso vediamo con uno split manuale di data, per vedere se il problema è quello.
train <- sample(1:nrow(data),floor(0.8*nrow(data)))
training_x <- as.matrix(data[train,X])
training_y <- data[train,"output"]
validation_x <- as.matrix(data[-train,X])
validation_y <- data[-train,"output"]

##Stesso modello di partenza.

storia <- modello %>% fit(
  training_x,
  training_y,
  validation_data= list(validation_x,validation_y),
  verbose = 1,
  epochs = 150,
  callbacks = early_stop
)

## SUL TRAINING SET R2
fits <- predict(modello, training_x)

## Non è male.
SSE <- sum((fits - training_y)^2)
DEV <- sum((training_y - mean(training_y))^2)
1 - SSE/DEV 

## SUL VALIDATION SET R2
fits <- predict(modello, validation_x)

## Non è male.
SSE <- sum((fits - validation_y)^2)
DEV <- sum((validation_y - mean(validation_y))^2)
1 - SSE/DEV 

## I modelli, su pochi dati, fanno decidsamente schifo.
```

```{r Conclusioni}

## Dati to Input è la relazione importante. Usando 5 anni, 2 osservazioni per giorno sono necessarie per ottenere buoni risultati

## Se troppo pochi dati, il modello non riesce a generalizzare (grosse differenze tra R2 di training e di testing).

## Soluzione: ridurre gli input o aumentare i dati a disposizione. Aumentare i dati sembra la via da seguire, visto che 24 osservazioni giornaliere sono la norma con i dati disponibili.
```

```{r Regolazione Reti Neurali}

tensorflow::set_random_seed(8119)
regulated <- c()
reps <- 3

layers <- 3
size <- c(200,200,200)

#max_t0 <- 45

drops <- seq(0,0.15, length.out = 5)
grid <- data.frame(dropout = drops)
# cl <- makeCluster(5, outfile ="")
# clusterExport(cl, c("create_model_dropout","early_stop","layers","size","X"))
# registerDoParallel(cl)

k = 0

CVs <- list(X = X)

system.time(
for (i in 365:(365+max_t0)) {
  #i = 365
  if((i - 365) %% retrain == 0) {
    
    k = k + 1
    cat("\nRegolazione numero:",k)
    
    data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
    
    output <- cv_nn(data_x[data_to_use,-days.pos], nsets, grid, time_wise = TRUE, reps)
    CVs[[k]] <- output
    
    mse_medi <- apply(output$results,2,mean)
    
    ##Elemento ottimale della griglia.
    opt <- which.min(mse_medi)
    regulated <- rbind(regulated, c(i,opt, as.numeric(grid[opt,])))
    
    ## Salviamo i parametri del modello.
    
  }
}
)


CVs[[1]]
## 1149 t. 20 minuti.
## 1183 t.

colnames(regulated) <- c("Giorno","Optimal","Dropout")
regulated.nn <- regulated

```

```{r Degenerazione Rete Neurale - Ciclo}
tensorflow::set_random_seed(22167)

#load("C:/Users/User/Documents/AI AGING/SIMULAZIONI/STAZIONARIETA/INTERAZIONE_con_ERRORE/regulated.nn")
regulated <- regulated.nn

regulated  ## Ottenuto da "Regolazione Reti Neurali"
reps=3

layers <- 3
size <- c(200,200,200)

#stopCluster(cl)
numCores <- detectCores()
cl <- makeCluster(numCores - 2)
clusterExport(cl, c("layers","size","X"))
registerDoParallel(cl)

set.seed(992)

system.time(
r <- foreach (i = days_selected, .packages = c("keras","tensorflow"), .export= c("create_model_dropout")) %dopar% {
  #i = 365
  
  early_stop <- callback_early_stopping(monitor = "val_mean_squared_error",
                                min_delta = 0,
                                patience = 8,
                                verbose = 0)
  
  ## Dati da usare
  data_to_use <- (data_x[,days.pos]>= (i - 364) & data_x[,days.pos]<=i)
  
  ## Aggiorniamo la struttura.
  reg_to_use <- sum(i >= regulated[,1])
  
  ## Creo i training e i test set.
  data_use <- data_x[data_to_use,]
  train <- sample(1:nrow(data_use), floor(nrow(data_use)*0.8))
  training_x <- as.matrix(data_use[train,X])
  validation_x <- as.matrix(data_use[-train,X])
  training_y <- data_use[train,"output"]
  validation_y <- data_use[-train,"output"]
  
 
  opt <- regulated[reg_to_use,2]
  
  ## Genero i seed
  seeds <- floor(runif(reps,min=1000, max=9999))
  models_reps <- rep(NA,reps)
  epochs_reps <- rep(NA,reps)
  
  for (j in 1:reps) {
    m.nn <- create_model_dropout(layers, size, length(X), grid[opt,1], seeds[j])
    
    storia <- m.nn %>% fit(x = training_x,
          y = training_y,
          batch_size = ceiling(nrow(training_x)/2),
          epochs = 1000,
          verbose = 1,
          validation_data = list(validation_x,validation_y),
          callbacks = early_stop)
    
    models_reps[j] <- storia$metrics$val_mean_squared_error[which.min(storia$metrics$val_mean_squared_error)]
    epochs_reps[j] <- which.min(storia$metrics$val_mean_squared_error)
  }
  
  ## Ristimo il modello migliore, usando il numero di epoche ottimali.
  seed <- seeds[which.min(models_reps)]
  epoche <- epochs_reps[which.min(models_reps)]
  
  m.nn <- create_model_dropout(layers, size, length(X), grid[opt,1], seed)
    
  storia <- m.nn %>% fit(x = training_x,
        y = training_y,
        batch_size = ceiling(nrow(training_x)/2),
        epochs = epoche,
        verbose = 1,
        validation_data = list(validation_x,validation_y),
        callbacks = early_stop)

  ## R2.
  fits.nn <- predict(m.nn, as.matrix(data_use[,X]), batch_size = 32, verbose = 0)
  SSE <- sum((data_use[,"output"] - fits.nn)^2)
  DEV <- sum((data_use[,"output"] - mean(data_use[,"output"]))^2)

  R2 <- 1 - SSE/DEV
  
  ##Elemento utilizzato.
  optimal_used <- regulated[reg_to_use,2]
  
  ## MSE a t0 mse_window.
  t0_train <- (data_x[,days.pos] > i & data_x[,days.pos] <= (i + mse_window))
  fits.t0 <- predict(m.nn, as.matrix(data_x[t0_train,X]), batch_size = 32, verbose = 0)
  mse.t0 <- mean((fits.t0 - data_x[t0_train,out.pos])^2)
  
  ## SALVO l'MSE a t0.
  
  for_fits <- data_x[,days.pos] >= (i + mse_window + 1) & data_x[,days.pos] < (i + mse_window + max_dt + finestra)
  to_predict <- data_x[for_fits,out.pos]
  to_predict_X <- as.matrix(data_x[for_fits,X])
  
  ## Mi salvo anche i giorni.
  days_to_use <- data_x[for_fits,days.pos]
  
  ##Tutte le previsioni da i + 1 fino a i + max_dt, finestra poi di finestra
  fits <- predict(m.nn, to_predict_X, batch_size = 32, verbose = 0)
  
  ## Finestre mobili
  
  prestazioni <- rep(NA,max_dt)
  for (j in 1:max_dt) {

    test <- (days_to_use >= (i + j + mse_window) & days_to_use < (i + j + mse_window + finestra))
    mse <- mean((to_predict[test] - fits[test])^2)
    
    prestazioni[j] <- mse
  }
  
  list(R2 = R2,epoche = epoche,optimal = optimal_used, mse_t0 = mse.t0, prestazioni = prestazioni)
  
}
)

stopCluster(cl)

for (j in 1:length(days_selected)) {
  models.attr[days_selected[j] - 364,"R2"] <- r[[j]]$R2
  models.attr[days_selected[j] - 364,"Grid_opt"] <- r[[j]]$optimal
  models.attr[days_selected[j] - 364,"MSE a t0"] <- r[[j]]$mse_t0
  models.attr[days_selected[j] - 364,"Epoche"] <- r[[j]]$epoche
  results[days_selected[j] - 364,] <- r[[j]]$prestazioni
}

r[[1]]$prestazioni
r[[2]]$prestazioni
r[[30]]$prestazioni

models.attr.nn <- models.attr
results.nn <- results
models.attr.nn <- as.data.frame(na.omit(models.attr.nn))
results.nn <- as.data.frame(na.omit(results.nn))

models.attr.nn <- as.data.frame(models.attr.nn)
models.attr.nn$R2_pred <- 1 - models.attr.nn$`MSE a t0`/MSSE_days_sel$MSSE_t0

```

```{r Plot Rete Neurale}

nn_plot <- function(models.attr, results, title) {
  
  ## Dataframed
  res.for.plot <- as.data.frame(t(results))
  res.for.plot$dt <- as.numeric(gsub("dt: ","",rownames(res.for.plot)))
  rownames(res.for.plot) <- NULL
  
  ## Togliamo 
  res.scale.for.plot <- res.for.plot

  ## Dividiamo gli MSE assoluti per renderli relativi.
  for (i in 1:nrow(models.attr)) {
    res.scale.for.plot[,i] <- res.scale.for.plot[,i]/models.attr[i,3]   ## Dividiamo per MSE a t0
  }

  ## Calcoliamo i quantili
  qt <- matrix(NA, nrow = max_dt, ncol = 4)
  colnames(qt) <- c("q25","q50","q75","dt")
  rownames(qt) <- 1:max_dt

  ## Popoliamo la matrice con i quantili
  for (i in 1:max_dt) {
    qt[i,] <- c(quantile(as.numeric(res.scale.for.plot[i,-ncol(res.scale.for.plot)]), probs = c(0.25,0.5,0.75)),i)
  }
  qt.for.plot <- as.data.frame(qt)

  ## Proviamo a plottare.

  ## Abbiamo bisogno dei singoli punti.
  res.scale.for.plot1 <- res.scale.for.plot[,c(1,ncol(res.scale.for.plot))]
  colnames(res.scale.for.plot1) <- c("values","dt")


  for (i in 2:(ncol(res.scale.for.plot)-1)) {
    tmp.df <- as.matrix(res.scale.for.plot[,c(i,ncol(res.scale.for.plot))])
    colnames(tmp.df) <- c("values","dt")
    res.scale.for.plot1 <- rbind(res.scale.for.plot1,tmp.df)
  }
  
  plot <- ggplot(data = res.scale.for.plot1, aes(x = dt, y = values)) + geom_point(stat="identity", col = blu_points, size = 0.8) +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q25), color = "yellow") +
  geom_line(data=qt.for.plot, mapping = aes(x = dt, y = q50), color = "black") +
  geom_line( data=qt.for.plot, mapping = aes(x = dt, y = q75), color = "red") +
  ylab("MSE Relativo") + xlab("dT, days after trained") + ggtitle(title)
  
  return(plot)

}

```



## SALVATAGGIO RISULTATI
```{r Salvataggio}

path <- "STAZIONARIETA_e_cambiamenti_temporali/COEFFICIENTI_variabili/stagionalita_tutti"

## Ridge
save(models.attr.ridge, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.ridge"))
save(results.ridge, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.ridge"))

## Random Forest
save(regulated.rf, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/regulated.rf"))
save(models.attr.rf, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.rf"))
save(results.rf, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.rf"))

## Gradient Boosting
save(regulated.gb, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/regulated.gb"))
save(models.attr.gb, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.gb"))
save(results.gb, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.gb"))

## Neural Network
save(regulated.nn, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/regulated.nn"))
save(models.attr.nn, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.nn"))
save(results.nn, file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.nn"))
```

```{r Grafici finali}

#path <- "STAZIONARIETA_e_cambiamenti_temporali/stagionalita_errore/variabili_per_cattura"
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.ridge"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.ridge"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.rf"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.rf"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.gb"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.gb"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/models.nn"))
load(file =paste0("C:/Users/User/Documents/AI AGING/SIMULAZIONI/",path,"/results.nn"))

cleaning <- function(obj) {
  as.data.frame(na.omit(obj))
}

models.attr.ridge <- cleaning(models.attr.ridge)
results.ridge <-  cleaning(results.ridge)
models.attr.rf <- cleaning(models.attr.rf)
results.rf <-  cleaning(results.rf)
models.attr.gb <- cleaning(models.attr.gb)
results.gb <-  cleaning(results.gb)
models.attr.nn <- cleaning(models.attr.nn)
results.nn <-  cleaning(results.nn)


blu_points <- rgb(198,219,239, max = 255, alpha=60)


ridge.p <- ridge_plot(models.attr.ridge, results.ridge, "Modello Ridge")
rf.p <- rf_plot(models.attr.rf, results.rf, "Modello RF")
gb.p <- gb_plot(models.attr.gb, results.gb, title = "Modello GB")
nn.p <- nn_plot(models.attr.nn, results.nn, title = "Modello NN")

plot_tutti <- list(ridge=ridge.p, rf = rf.p, gb = gb.p, nn=nn.p)

ridge.p + ylim(0.7,1.5)
#plot_ARMA_media$ridge + ylim(0.7,1.5)
rf.p + ylim(0.7,1.5)
#plot_ARMA_media$rf + ylim(0.7,1.5)
gb.p + ylim(0.5,1.8)
#plot_ARMA_media$gb + ylim(0.7,1.5)
nn.p + ylim(0.8,1.35)
#plot_ARMA_media$nn + ylim(0.7,1.5)


plot_ARMA_media <- list(ridge=ridge.p, rf = rf.p, gb = gb.p, nn=nn.p)
```

```{r Plot andamento R2}
R2.plot <- data.frame(days = as.numeric(days_selected) - 364)
R2.plot$ridge <- models.attr.ridge$R2_pred
R2.plot$rf <- models.attr.rf$R2_pred
R2.plot$gb <- models.attr.gb$R2_pred
R2.plot$nn <- models.attr.nn$R2_pred

col <- palette.colors(n=4, palette="r4")

plot_r2 <- ggplot(data = R2.plot, aes(x = days)) +
  geom_line(aes(y = ridge, color = "Ridge")) +
  geom_line(aes(y = rf, color = "RF")) +
  geom_line(aes(y = gb, color = "GB")) +
  geom_line(aes(y = nn, color = "NN")) +
  scale_color_manual(
    values = c("Ridge" = col[1], "RF" = col[2], "GB" = col[3], "NN" = col[4]),
    labels = c("Ridge", "RF", "GB", "NN"),
    breaks = c("Ridge", "RF", "GB", "NN")
  ) + ggtitle("R2 previsivo dei modelli nel tempo")

plot_r2
```

















